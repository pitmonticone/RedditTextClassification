{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tranformers\n",
    "\n",
    "In this notebooj we try the bert base uncased spaCy implementation. We laso tried pytt with less sucess (not compatible).  \n",
    "See [documentation](https://github.com/explosion/spacy-transformers) and references: \n",
    "1. [train_textcat](https://github.com/explosion/spacy-transformers/blob/master/examples/train_textcat.py )\n",
    "2. [tranformers](https://code.ihub.org.cn/projects/763/repository/transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q:\\anaconda\\envs\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "Q:\\anaconda\\envs\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "Q:\\anaconda\\envs\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "Q:\\anaconda\\envs\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "Q:\\anaconda\\envs\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "Q:\\anaconda\\envs\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#####run only once (the first time)\n",
    "#!pip install --upgrade spacy #incompatible with (!pip install spacy-pytorch-transformers)\n",
    "#!pip install torch===1.4.0 torchvision===0.5.0 -f https://download.pytorch.org/whl/torch_stable.html  #this is useful only on jupyter\n",
    "#!pip uninstall spacy-pytorch-transformers #this fixes \n",
    "#!pip install spacy-transformers  #no, keep it commentedon kaggle\n",
    "#!pip install spacy-pytorch-transformers #it is deprecated https://github.com/explosion/spaCy/issues/4382\n",
    "#!python -m spacy download en_trf_bertbaseuncased_lg  \n",
    "#####\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import math   \n",
    "from spacy.util import minibatch , compounding \n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from progressbar import ProgressBar, Bar, Percentage\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.utils import shuffle\n",
    "from spacy_transformers.util import cyclic_triangular_rate\n",
    "from collections import Counter\n",
    "\n",
    "# is_using_gpu = spacy.prefer_gpu()\n",
    "# if is_using_gpu:\n",
    "#     torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "#     print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slightly moidified roc evaluation\n",
    "def evaluate_roc(nlp, textcat):\n",
    "    print(\"evaluate1\")\n",
    "    docs = [nlp(tex) for tex in test_texts]\n",
    "    print(\"evaluate2\")\n",
    "    y_pred = [doc.cats[\"1\"] for doc in docs]\n",
    "    #y_pred = [b[0]  for b in scores]\n",
    "    print(\"evaluate3\")\n",
    "    roc = roc_auc_score(test_labels, y_pred)\n",
    "    print(\"evaluate4\")\n",
    "    return roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_target_full = pd.read_csv(\"Q:\\\\tooBigToDrive\\data-mining\\kaggle\\data\\csv\\\\train_target.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# this part enables lPunctAgg lemmatization.\n",
    "path = r\"Q:\\tooBigToDrive\\data-mining\\kaggle\\data\\csv\"\n",
    "files = listdir(path)\n",
    "files = [f for f in files if \"Agg\" in f and not \".npy\" in f]\n",
    "# actaully we only did this one for computational tume reasons (~14 hours for two epochs)\n",
    "#files = [\"lPunctNumStopLemOovAlphaSentAgg.csv\"]\n",
    "for f in files:\n",
    "    print(\"doing: \", f)\n",
    "    #lemmed = lPunctAgg\n",
    "    train_data_full = pd.read_csv(path+\"\\\\\"+f)\n",
    "    train_data_full[\"body\"] = train_data_full[\"subreddit\"]+\" \"+train_data_full[\"body\"]\n",
    "    # print(\"dropping subreddits and utc\")\n",
    "    # train_data_full.drop([\"created_utc\",\"subreddit\"], axis = 1, inplace = True)\n",
    "    print(\"len(train_data_full) before elimitating empties = \",len(train_data_full), \"train_data_full.loc[0,'body'] = \", train_data_full.loc[0,'body'])\n",
    "    # enable following line to exclude rows with less than 4 words\n",
    "    #train_data_full = train_data_full[train_data_full.body.count(\" \") >3]\n",
    "    train_data_full.replace(to_replace = \"empty\", value = \"\", inplace = True)\n",
    "    train_data_full[\"body\"].fillna(\"\",inplace = True)\n",
    "    train_data_full.reset_index(drop  = True, inplace = True)\n",
    "    print(\"len(train_data_full) after elimitating empties = \",len(train_data_full))\n",
    "    print(\"creating gender...\")\n",
    "\n",
    "    ###### end of the part to be commented to avoid lPunctAgg lemmatization\n",
    "\n",
    "\n",
    "    # common part to all procedures. This should never be commented\n",
    "\n",
    "    gender = [0 for i in range(len(train_data_full))]\n",
    "    print(\"len(gender) = \", len(gender))\n",
    "    # for idx, row in train_data_full.iterrows():\n",
    "    #     gender.append(train_target_full[train_target_full[\"author\"] == row.author].iloc[0,1])\n",
    "    print(\"gender created. Adding subreddits...\")\n",
    "\n",
    "\n",
    "    print(\"populating gender list...\")\n",
    "    for idx, row in train_target_full.iterrows():\n",
    "        if row.gender == 1:\n",
    "            #print(\"inside th if\")\n",
    "            indexes = train_data_full.index[train_data_full[\"author\"] == row.author].tolist()\n",
    "            for i in indexes:\n",
    "                #print(\"inside the for\")\n",
    "                gender[i] += 1\n",
    "    print(\"gender list polulated. Creating n_words column for future use...\")\n",
    "    #print(fake_gender)\n",
    "    if(len(np.unique(gender) == 2)):\n",
    "        train_data_full[\"gender\"] = gender\n",
    "    else:\n",
    "        print(\"there has been an error with gender recognition, please halt the program now\")\n",
    "\n",
    "    ### end of the common part\n",
    "\n",
    "\n",
    "    # this is the balanced part\n",
    "\n",
    "#     train_data_full_m = train_data_full.loc[train_data_full[\"gender\"] == 0, :]\n",
    "#     train_data_full_f = train_data_full.loc[train_data_full[\"gender\"] == 1, :]\n",
    "\n",
    "#     split = math.floor(len(train_data_full_f)*0.8)\n",
    "#     print(\"split = \",split)\n",
    "\n",
    "\n",
    "\n",
    "#     train_data_sample_m =  train_data_full_m.sample(split)\n",
    "#     train_texts_m =train_data_sample_m[\"body\"].tolist()\n",
    "#     test_data_sample_m = train_data_full_m.drop(train_data_sample_m.index)\n",
    "#     test_texts_m = test_data_sample_m[\"body\"].tolist()\n",
    "\n",
    "#     train_data_sample_f =  train_data_full_f.sample(split)\n",
    "#     train_texts_f = train_data_sample_f[\"body\"].tolist()\n",
    "#     test_data_sample_f = train_data_full_f.drop(train_data_sample_f.index)\n",
    "#     test_texts_f = test_data_sample_f[\"body\"].tolist()\n",
    "#     print(\"len(test_texts_f) = \", len(test_texts_f), \"len(test_texts_m) = \", len(test_texts_m))\n",
    "#     train_texts = train_texts_m + train_texts_f\n",
    "#     test_texts = test_texts_m + test_texts_f\n",
    "\n",
    "#     train_labels = [{'cats': {'1': False, '0': True}} for i in range(split)] + [{'cats': {'1': True, '0': False}} for i in range(split)]\n",
    "#     test_labels = [0 for i in range(len(train_data_full_m)-split)] + [1 for i in range(len(train_data_full_f)-split)]\n",
    "\n",
    "#     train_labels_reg = [0 for i in range(split)] + [1 for j in range(split)]\n",
    "\n",
    "    #train_texts , train_labels_reg = shuffle(train_texts , train_labels_reg , random_state = 0)\n",
    "\n",
    "    # end of balanced part\n",
    "\n",
    "    # unbalanced part\n",
    "    \n",
    "    seed = 100\n",
    "    split = math.floor(len(train_data_full)*0.8)\n",
    "    print(\"split = \",split)\n",
    "\n",
    "    \n",
    "    train_df = train_data_full.sample(split, random_state = 100)\n",
    "    print(\"len(train_df) before removing blanks = \", len(train_df))\n",
    "    train_df = train_df[train_df[\"body\"].str.strip() != \"\"]\n",
    "    print(\"len(train_df) after removing blanks = \", len(train_df))\n",
    "    test_df = train_data_full.drop(train_df.index)\n",
    "    print(\"len(test_df) before removing blanks = \", len(test_df))\n",
    "    test_df = test_df[test_df[\"body\"].str.strip() != \"\"]\n",
    "    print(\"len(test_df) after removing blanks = \", len(test_df))\n",
    "    \n",
    "    train_texts  = train_df[\"body\"].tolist()\n",
    "    test_texts =  test_df[\"body\"].tolist()\n",
    "    print(\"len(test_texts) before removing blanks = \", len(test_texts))\n",
    "    #test_texts = [t if t not \"\" for t in test_texts]\n",
    "    print(\"len(test_texts) after removing blanks = \", len(test_texts))\n",
    "\n",
    "    train_labels = [{'cats': {'1': label == 1,'0': label == 0}} for label in train_df[\"gender\"].tolist()]\n",
    "    test_labels = [i for i in test_df[\"gender\"].tolist()]  #[0 for i in range(len(train_data_full_m)-split)] + [1 for i in range(len(train_data_full_f)-split)]\n",
    "    \n",
    "    # end of unbalanced part\n",
    "    \n",
    "    # orsenigo-compatible unbalanced\n",
    "#     train_texts, test_texts, train_labels, test_labels = train_test_split(train_data_full[\"body\"].tolist(),train_data_full[\"gender\"].tolist() ,train_size=0.8, test_size=0.2,\n",
    "#                                                       random_state=0)\n",
    "#     train_labels = [{'cats': {'1': label == 1,'0': label == 0}} for label in train_labels]\n",
    "    \n",
    "    # end of davide-compatible unvalanced\n",
    "\n",
    "\n",
    "#     # common part to all approaches : do not comment\n",
    "\n",
    "    print(\"len(train_texts) = \",len(train_texts),\"len(test_texts) = \" , len(test_texts))\n",
    "\n",
    "\n",
    "\n",
    "    train_data  = list(zip(train_texts, train_labels))\n",
    "    print(\"len(test_texts) == len(test_labels) : \", len(test_texts) == len(test_labels) )\n",
    "    print(\"len(train_data) = \",len(train_data), \"len(test_data) = \", len(test_texts))\n",
    "    max_wpb = max([text.count(\" \") for text in train_texts + test_texts])\n",
    "    print(\"max_wpb = \", max_wpb)\n",
    "\n",
    "    # end of common part to all approaches\n",
    "    \n",
    "    #you have to locate where it is (https://stackoverflow.com/questions/54334304/spacy-cant-find-model-en-core-web-sm-on-windows-10-and-python-3-5-3-anacon)\n",
    "    nlp = spacy.load(r'Q:\\anaconda\\Lib\\site-packages\\en_trf_bertbaseuncased_lg\\en_trf_bertbaseuncased_lg-2.2.0') #use this script to look for files:  print(os.listdir(\"/opt/conda/lib/python3.6/site-packages/en_trf_bertbaseuncased_lg/en_trf_bertbaseuncased_lg-2.2.0\"))\n",
    "    print(nlp.pipe_names) # [\"sentencizer\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    if 'trf_textcat' not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\"trf_textcat\", config={\"exclusive_classes\": True , \"architecture\": \"softmax_last_hidden\", \"words_per_batch\": max_wpb}) #, <\"architecture\":\"bow\"> would not work #\"architecture\": \"softmax_last_hidden\", \"words_per_batch\": max_wpb dpes not wprk\n",
    "        print(\"pheww\")\n",
    "        textcat.add_label(\"1\")\n",
    "        textcat.add_label(\"0\")\n",
    "        nlp.add_pipe(textcat, last = True)\n",
    "    else:\n",
    "        print(\"something went wrong\")\n",
    "        textcat = nlp.get_pipe('trf_textcat')\n",
    "        # tried teh sentncizer,as per \n",
    "    #nlp.add_pipe(nlp.create_pipe(\"sentencizer\"))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    random.seed(1)\n",
    "    spacy.util.fix_random_seed(1)\n",
    "\n",
    "    # write it otherwise it is not happy\n",
    "    nlp.vocab.vectors.name = 'spacy_pretrained_vectors'\n",
    "\n",
    "    # speaks for itself\n",
    "    print(\"random seeds set\")\n",
    "    name = f.replace(\".csv\",\"\")\n",
    "    losses = Counter()\n",
    "    #losses  = {}\n",
    "    rocs = []\n",
    "    run_title = \"softmax_bert_sent_\"+name\n",
    "    output = \"\"\n",
    "\n",
    "    print(\"strings and lists initialized\")\n",
    "\n",
    "\n",
    "    #learning process\n",
    "    #for batch_size in [10, 50, 100, 200, 300] : \n",
    "    pipe_exceptions = [\"sentencizer\",'trf_textcat', \"trf_wordpiecer\",\"trf_tok2vec\"] #(\"trf_wordpiecer\", \"trf_tok2vec) are required, #https://github.com/explosion/spaCy/blob/master/examples/training/train_textcat.py\n",
    "    print(\"pipe_exceptions defined\")\n",
    "    print(nlp.pipe_names)\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    print(\"other_pipes defined\")\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "        optimizer = nlp.resume_training()\n",
    "        optimizer.alpha = 0.001\n",
    "        optimizer.trf_weight_decay = 0.005\n",
    "        optimizer.L2 = 0.0\n",
    "        learn_rate=2e-5\n",
    "        batch_size  = 8\n",
    "        learn_rates = cyclic_triangular_rate(learn_rate / 3, learn_rate * 3, 2 * len(train_data) // batch_size)\n",
    "        #learn_rates = cyclic_triangular_rate(learn_rate / 3, learn_rate * 3, 2 * len(train_data) // batch_size)\n",
    "        for epoch in range(2):\n",
    "            random.shuffle(train_data)\n",
    "            print(\"data shuffled\")\n",
    "            # Create the batch generator with batch size = 8\n",
    "            batches = minibatch(train_data, size=batch_size)\n",
    "            print(\"batches created\")\n",
    "            optimizer.trf_lr = next(learn_rates)\n",
    "            # Iterate through minibatches\n",
    "            pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=499).start()  #8387 is the total number of iterations needed for an unbalanced 0.8 trainset. 0.04 of unbalanced would be approx 1540 . 4595 for lemmatized balanced. 8420 for lemmatized unbalanced\n",
    "            i = 0                                                                    # 737 for  unbalanced, 466 for balanced\n",
    "            for batch in batches:\n",
    "                texts1, labels = zip(*batch)\n",
    "                nlp.update(texts1, labels, sgd = optimizer, losses=losses, drop = 0.1)\n",
    "                i += 1\n",
    "                pbar.update(i)\n",
    "            pbar.finish()\n",
    "            print(\"i = \",  i)\n",
    "            #with textcat.model.use_params(optimizer.averages):   #'bool' object has no attribute 'use_params' \n",
    "            rocs.append(evaluate_roc(nlp, textcat))\n",
    "            output += f\"    epoch = {epoch}, roc = {rocs[-1]} \\n \"  # losses = {losses},\n",
    "            print( \"epoch = \",epoch, \"roc = \" , rocs[-1] , \"i = \", i)  #\" losses = \", losses\n",
    "                \n",
    "                \n",
    "                \n",
    "    with open(\"outputs\\\\\" + run_title + \".txt\", \"a\") as file: #name\n",
    "        file.write(run_title +\"\\n\" + output)\n",
    "        file.close()\n",
    "        \n",
    "      # save model\n",
    "#     nlp.to_disk(r\"Q:\\tooBigToDrive\\data-mining\\kaggle\\my_models\\spaCy\\savedModels\\lPNSLO_bert_softmax\")\n",
    "#     print(\"Saved model to\", r\"Q:\\tooBigToDrive\\data-mining\\kaggle\\my_models\\spaCy\\savedModels\\lPNSLO_bert_softmax\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myEnv]",
   "language": "python",
   "name": "conda-env-myEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

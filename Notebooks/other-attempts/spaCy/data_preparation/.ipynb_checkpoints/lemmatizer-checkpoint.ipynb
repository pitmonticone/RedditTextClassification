{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Lemmatizer\n",
    "\n",
    "This notebook is the first il logical order. It takes the challenge data as input and produces various preprocessings/lemmatizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from progressbar import ProgressBar, Bar, Percentage\n",
    "import numpy as np\n",
    "import re \n",
    "import string as stri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be verified that lg and bert models hve the same stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "nlplg  = spacy.load(r\"Q:\\anaconda\\Lib\\site-packages\\en_core_web_lg\\en_core_web_lg-2.2.5\", disable = [\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_data_full = pd.read_csv(r\"train_data.csv\")\n",
    "train_target_full = pd.read_csv(r\"train_target.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercase subreddit column\n",
    "train_data_full[\"subreddit\"]  = list(map(lambda x: x.lower() , train_data_full[\"subreddit\"].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to process subreddits\n",
    "def proc_subs(l):\n",
    "    s = set(l)\n",
    "    return \" \".join([st.lower() for st in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create merged df\n",
    "train_data_full_agg = train_data_full.groupby([\"author\"], as_index = False).agg({'subreddit':  proc_subs, \"body\": \" \".join})\n",
    "\n",
    "# bodies to be preprocessed/lemmatized, both in indivdual and aggregate form\n",
    "to_be_lemmed = train_data_full[\"body\"].tolist()\n",
    "to_be_lemmed_agg = train_data_full_agg[\"body\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three different function for simple preprocesisng\n",
    "# substitutes !,? and . with . to enable sentencizing, and removes punctuation and numbers \n",
    "def preprocessPunctNumSent(tex):\n",
    "    tex = tex.replace(\"\\r\",\"\")\n",
    "    tex = tex.replace(\"\\\\n\",\".\")   #\n",
    "    tex = tex.replace(\"\\n\",\".\")\n",
    "    tex = tex.translate(str.maketrans(\"\",\"\", '\"#$%&\\'()*+,-/:<=>@[\\\\]^_`{|}~')).translate(str.maketrans('','','1234567890')).lower().strip()\n",
    "    tex = re.sub(r\"\\!+\", \".\", tex)\n",
    "    tex = re.sub(r\";+\", \".\", tex)\n",
    "    tex = re.sub(r\"\\?+\", \".\", tex)\n",
    "    tex = re.sub(r\"(\\.+\\s*)+\", \". \", tex)\n",
    "    tex = \" \".join(tex.split())\n",
    "    return tex\n",
    "\n",
    "\n",
    "# removes punctuation and numbers\n",
    "def preprocessPunctNum(tex):\n",
    "    tex = tex.replace(\"\\r\",\"\")\n",
    "    tex = tex.replace(\"\\\\n\",\"\")   #\n",
    "    tex = tex.replace(\"\\n\",\"\")\n",
    "    tex = tex.translate(str.maketrans(\"\",\"\", stri.punctuation)).translate(str.maketrans('','','1234567890')).lower().strip()\n",
    "    tex = \" \".join(tex.split())\n",
    "    return tex\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# only removes punctuation and \\ characters\n",
    "def preprocessPunct(tex):\n",
    "    tex = tex.replace(\"\\r\",\"\")\n",
    "    tex = tex.replace(\"\\\\n\",\"\")\n",
    "    tex = tex.replace(\"\\n\",\"\")\n",
    "    tex = tex.translate(str.maketrans(\"\",\"\", stri.punctuation)).lower().strip()\n",
    "    tex = \" \".join(tex.split())\n",
    "    return tex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "j = 0\n",
    "texts = to_be_lemmed\n",
    "pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=len(texts)).start()\n",
    "for tex in texts:\n",
    "    docs.append(nlplg(preprocessPunctNum(tex)))\n",
    "    j += 1\n",
    "    pbar.update(j)\n",
    "pbar.finish()\n",
    "\n",
    "docs_agg = []\n",
    "j = 0\n",
    "texts = to_be_lemmed_agg\n",
    "pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=len(texts)).start()\n",
    "for tex in texts:\n",
    "    docs.append(nlplg(preprocessPunctNum(tex)))\n",
    "    j += 1\n",
    "    pbar.update(j)\n",
    "pbar.finish()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different functions for different lemmatizations/ further preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only removes punctuation and \\ characters and oov words\n",
    "def lemmatize0(texts):\n",
    "    final_l = []\n",
    "    print(\"lemmatize0\")\n",
    "    pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=len(texts)).start()\n",
    "    j = 0\n",
    "    for tex in texts:\n",
    "        final_l.append(preprocessPunct(tex))\n",
    "        pbar.update(j)\n",
    "        j += 1\n",
    "    pbar.finish()\n",
    "    return final_l\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# like lemmatize0 but also remove numbers\n",
    "def lemmatize1(docs):\n",
    "    final_l = []\n",
    "    print(\"lemmatize1\")\n",
    "    pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=len(docs)).start()\n",
    "    i = 0\n",
    "    for doc in docs:\n",
    "        final = \"\"\n",
    "        for token in doc:\n",
    "            if token.pos_ not in [\"SPACE\"] and token.lemma_ not in [\"-PRON-\"]:\n",
    "                final += token.lemma_ + \" \"\n",
    "        final_l.append(final.strip())\n",
    "        i += 1\n",
    "        pbar.update(i)\n",
    "    pbar.finish()\n",
    "    return final_l\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#like lemmatize 1 but also remove oov \n",
    "def lemmatize11(docs):\n",
    "    final_l = []\n",
    "    print(\"lemmatize11\")\n",
    "    pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=len(docs)).start()\n",
    "    i = 0\n",
    "    for doc in docs:\n",
    "        final = \"\"\n",
    "        for token in doc:\n",
    "            if not token.is_oov and token.pos_ not in [\"SPACE\"] and token.lemma_ not in [\"-PRON-\"]:\n",
    "                final += token.lemma_ + \" \"\n",
    "        final_l.append(final.strip())\n",
    "        i += 1\n",
    "        pbar.update(i)\n",
    "    pbar.finish()\n",
    "    return final_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# like lemmatize1 but alse remove stopwords\n",
    "def lemmatize2(docs):\n",
    "    final_l = []\n",
    "    print(\" lemmatize2\")\n",
    "    pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=len(docs)).start()\n",
    "    i = 0\n",
    "    for doc in docs:\n",
    "        final = \"\"\n",
    "        for token in doc:\n",
    "            if not token.is_stop and token.pos_ not in [\"SPACE\"]:\n",
    "                final += token.lemma_ + \" \"\n",
    "        final_l.append(final.strip())\n",
    "        i += 1\n",
    "        pbar.update(i)\n",
    "    pbar.finish()\n",
    "    return final_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lik lemmetize2 but alse remove oov\n",
    "def lemmatize21(docs):\n",
    "    final_l = []\n",
    "    print(\"lemmatize21\")\n",
    "    pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=len(docs)).start()\n",
    "    i = 0\n",
    "    for doc in docs:\n",
    "        final = \"\"\n",
    "        for token in doc:\n",
    "            if not token.is_oov and token.pos_ not in [\"SPACE\"] and not token.is_stop :\n",
    "                final += token.text + \" \"\n",
    "        final_l.append(final.strip())\n",
    "        i += 1\n",
    "        pbar.update(i)\n",
    "    pbar.finish()\n",
    "    return final_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lik lemmetize21 but alse remove non alpha but laves \".\" for sentencizer (optional, see blow)\n",
    "def lemmatize22(docs):\n",
    "    final_l = []\n",
    "    print(\"lemmatize22\")\n",
    "    pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=len(docs)).start()\n",
    "    i = 0\n",
    "    for doc in docs:\n",
    "        final = \"\"\n",
    "        for token in doc:\n",
    "            if not token.is_oov and token.pos_ not in [\"SPACE\"] and not token.is_stop and token.is_alpha:# add this to enable Sent:  or token.text == \".\" \n",
    "                if token.text != \".\":\n",
    "                    final += token.lemma_ + \" \"\n",
    "                else:\n",
    "                    final = final.rstrip()\n",
    "                    final += \". \"\n",
    "        final_l.append(final.strip())  # add this to enable Sent: re.sub(r\"(\\.+\\s*)+\", \". \", final).strip()\n",
    "        i += 1\n",
    "        pbar.update(i)\n",
    "    pbar.finish()\n",
    "    return final_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# like lemmatize2 , but no longer removes stopwrds. It removes all pronouns, auxliliaries, words like \"how, what, why etc\"\n",
    "def lemmatize3(docs):\n",
    "    final_l = []\n",
    "    print(\"lemmatize3\")\n",
    "    pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=len(docs)).start()\n",
    "    i = 0\n",
    "    for doc in docs:\n",
    "        final = \"\"\n",
    "        for token in doc:\n",
    "            if token.pos_ not in [\"SPACE\",\"PRON\",\"DET\",\"AUX\",\"ADP\"] and token.dep_ not in [\"WRB\"]:\n",
    "                final += token.text + \" \"\n",
    "        final_l.append(final.strip())\n",
    "        i += 1\n",
    "        pbar.update(i)\n",
    "    pbar.finish()\n",
    "    return final_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# like lemmatize2 , but no longer removes stopwrds. It removes all pronouns, auxliliaries, words like \"how, what, why etc\" and misspelled words\n",
    "def lemmatize31(docs):\n",
    "    final_l = []\n",
    "    print(\"lemmatize31\")\n",
    "    pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=len(docs)).start()\n",
    "    i = 0\n",
    "    for doc in docs:\n",
    "        final = \"\"\n",
    "        for token in doc:\n",
    "            if token.pos_ not in [\"SPACE\",\"PRON\",\"DET\",\"AUX\",\"ADP\"] and token.dep_ not in [\"WRB\"] and not token.is_oov: \n",
    "                final += token.text + \" \"\n",
    "        final_l.append(final.strip())\n",
    "        i += 1\n",
    "        pbar.update(i)\n",
    "    pbar.finish()\n",
    "    return final_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wh-adverb\n",
      "adverbial modifier\n",
      "adverb\n",
      "pronoun, possessive\n",
      "interjection\n",
      "other\n",
      "space\n",
      "noun, singular or mass\n",
      "verb, past participle\n",
      "noun, proper singular\n",
      "proper noun\n",
      "superfluous punctuation\n",
      "object of preposition\n",
      "direct object\n",
      "complement of preposition\n",
      "adposition\n",
      "adjective\n",
      "compound\n",
      "particle\n"
     ]
    }
   ],
   "source": [
    "# this way one may understand spaCy abrreviations\n",
    "print(spacy.explain(\"WRB\"))\n",
    "print(spacy.explain(\"advmod\"))\n",
    "print(spacy.explain(\"RB\"))\n",
    "print(spacy.explain(\"PRP$\"))\n",
    "print(spacy.explain(\"INTJ\"))\n",
    "print(spacy.explain(\"X\"))\n",
    "print(spacy.explain(\"SPACE\"))\n",
    "print(spacy.explain(\"NN\"))\n",
    "print(spacy.explain(\"VBN\"))\n",
    "print(spacy.explain(\"NNP\"))\n",
    "print(spacy.explain(\"PROPN\"))\n",
    "print(spacy.explain(\"NFP\"))\n",
    "print(spacy.explain(\"pobj\"))\n",
    "print(spacy.explain(\"dobj\"))\n",
    "print(spacy.explain(\"pcomp\"))\n",
    "print(spacy.explain(\"ADP\"))\n",
    "print(spacy.explain(\"JJ\"))\n",
    "print(spacy.explain(\"compound\"))\n",
    "print(spacy.explain(\"PART\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists of interest\n",
    "authors = train_data_full[\"author\"].tolist()\n",
    "authors_agg = train_data_full_agg[\"author\"].tolist()\n",
    "\n",
    "subreddits = train_data_full[\"subreddit\"].tolist()\n",
    "subreddits_agg = train_data_full_agg[\"subreddit\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce all lemmatizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer0\n",
    "\n",
    "\n",
    "lemmed  = lemmatize0(to_be_lemmed )\n",
    "\n",
    "print(\"writing to file\")\n",
    "df = pd.DataFrame({\"author\": authors , \"subreddit\" : subreddits, \"body\" : lemmed})\n",
    "df.to_csv(r\"Q:\\\\tooBigToDrive\\data-mining\\kaggle\\data\\lPunct.csv\", index=False)\n",
    "print(\"done\")\n",
    "    \n",
    "lemmed_agg = lemmatize0(to_be_lemmed_agg)\n",
    "print(\"writing to file\")\n",
    "df = pd.DataFrame({\"author\": authors_agg , \"subreddit\" : subreddits_agg, \"body\" : lemmed_agg})\n",
    "df.to_csv(r\"Q:\\\\tooBigToDrive\\data-mining\\kaggle\\data\\lPunctAgg.csv\", index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer1\n",
    "lemmed  = lemmatize1(to_be_lemmed )\n",
    "print(\"writing to file\")\n",
    "df = pd.DataFrame({\"author\": authors , \"subreddit\" : subreddits, \"body\" : lemmed})\n",
    "df.to_csv(r\"Q:\\\\tooBigToDrive\\data-mining\\kaggle\\data\\lPunctNumLem.csv\", index=False)\n",
    "print(\"done\")\n",
    "\n",
    "lemmed_agg = lemmatize1(to_be_lemmed_agg)\n",
    "print(\"writing to file\")\n",
    "df = pd.DataFrame({\"author\": authors_agg , \"subreddit\" : subreddits_agg, \"body\" : lemmed_agg})\n",
    "df.to_csv(r\"Q:\\\\tooBigToDrive\\data-mining\\kaggle\\data\\lPunctNumLemAgg.csv\", index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmed  = lemmatize11(to_be_lemmed )\n",
    "print(\"writing to file\")\n",
    "df = pd.DataFrame({\"author\": authors , \"subreddit\" : subreddits, \"body\" : lemmed})\n",
    "df.to_csv(r\"Q:\\\\tooBigToDrive\\data-mining\\kaggle\\data\\lPunctNumLemOov.csv\", index=False)\n",
    "print(\"done\")\n",
    "\n",
    "lemmed_agg = lemmatize11(to_be_lemmed_agg)\n",
    "print(\"writing to file\")\n",
    "df = pd.DataFrame({\"author\": authors_agg , \"subreddit\" : subreddits_agg, \"body\" : lemmed_agg})\n",
    "df.to_csv(r\"Q:\\\\tooBigToDrive\\data-mining\\kaggle\\data\\lPunctNumLemOovAgg.csv\", index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer2\n",
    "lemmed  = lemmatize2(to_be_lemmed )\n",
    "print(\"writing to file\")\n",
    "df = pd.DataFrame({\"author\": authors , \"subreddit\" : subreddits, \"body\" : lemmed})\n",
    "df.to_csv(r\"Q:\\\\tooBigToDrive\\data-mining\\kaggle\\data\\lPunctNumStop.csv\", index=False)\n",
    "print(\"done\")\n",
    "\n",
    "lemmed_agg = lemmatize2(nlplg,to_be_lemmed_agg)\n",
    "print(\"writing to file\")\n",
    "df = pd.DataFrame({\"author\": authors_agg , \"subreddit\" : subreddits_agg, \"body\" : lemmed_agg})\n",
    "df.to_csv(r\"Q:\\\\tooBigToDrive\\data-mining\\kaggle\\data\\lPunctNumStopAgg.csv\", index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize21\n",
    "lemmed  = lemmatize21(to_be_lemmed )\n",
    "print(\"writing to file\")\n",
    "df = pd.DataFrame({\"author\": authors , \"subreddit\" : subreddits, \"body\" : lemmed})\n",
    "df.to_csv(r\"Q:\\\\tooBigToDrive\\data-mining\\kaggle\\data\\lPunctNumStopLemOov.csv\", index=False)\n",
    "print(\"done\")\n",
    "\n",
    "lemmed_agg = lemmatize21(to_be_lemmed_agg)\n",
    "print(\"writing to file\")\n",
    "df = pd.DataFrame({\"author\": authors_agg , \"subreddit\" : subreddits_agg, \"body\" : lemmed_agg})\n",
    "df.to_csv(r\"Q:\\\\tooBigToDrive\\data-mining\\kaggle\\data\\lPunctNumStopOovAgg.csv\", index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatize22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to file\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "lemmatize22\n",
    "lemmed  = lemmatize22(to_be_lemmed )\n",
    "print(\"writing to file\")\n",
    "df = pd.DataFrame({\"author\": authors , \"subreddit\" : subreddits, \"body\" : lemmed})\n",
    "df.to_csv(r\"Q:\\\\tooBigToDrive\\data-mining\\kaggle\\data\\lPunctNumStopLemOovAlphaSent.csv\", index=False)\n",
    "print(\"done\")\n",
    "\n",
    "lemmed_agg = lemmatize22(to_be_lemmed_agg)\n",
    "print(\"writing to file\")\n",
    "df = pd.DataFrame({\"author\": authors_agg , \"subreddit\" : subreddits_agg, \"body\" : lemmed_agg})\n",
    "df.to_csv(r\"Q:\\\\tooBigToDrive\\data-mining\\kaggle\\data\\csv\\lPunctNumStopLemOovAlphaAgg.csv\", index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize3\n",
    "lemmed  = lemmatize3(to_be_lemmed )\n",
    "print(\"writing to file\")\n",
    "df = pd.DataFrame({\"author\": authors , \"subreddit\" : subreddits, \"body\" : lemmed})\n",
    "df.to_csv(r\"Q:\\\\tooBigToDrive\\data-mining\\kaggle\\data\\lPunctNumPers.csv\", index=False)\n",
    "print(\"done\")\n",
    "\n",
    "lemmed_agg = lemmatize3(to_be_lemmed_agg)\n",
    "print(\"writing to file\")\n",
    "df = pd.DataFrame({\"author\": authors_agg , \"subreddit\" : subreddits_agg, \"body\" : lemmed_agg})\n",
    "df.to_csv(r\"Q:\\\\tooBigToDrive\\data-mining\\kaggle\\data\\lPunctNumPersAgg.csv\", index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize31\n",
    "lemmed  = lemmatize3(to_be_lemmed )\n",
    "print(\"writing to file\")\n",
    "df = pd.DataFrame({\"author\": authors , \"subreddit\" : subreddits, \"body\" : lemmed})\n",
    "df.to_csv(r\"Q:\\\\tooBigToDrive\\data-mining\\kaggle\\data\\lPunctNumPersOov.csv\", index=False)\n",
    "print(\"done\")\n",
    "\n",
    "lemmed_agg = lemmatize3(nlplg,to_be_lemmed_agg)\n",
    "print(\"writing to file\")\n",
    "df = pd.DataFrame({\"author\": authors_agg , \"subreddit\" : subreddits_agg, \"body\" : lemmed_agg})\n",
    "df.to_csv(r\"Q:\\\\tooBigToDrive\\data-mining\\kaggle\\data\\lPunctNumPersOovAgg.csv\", index=False)\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myEnv]",
   "language": "python",
   "name": "conda-env-myEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

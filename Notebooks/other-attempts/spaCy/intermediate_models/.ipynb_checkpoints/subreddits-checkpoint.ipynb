{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration with subreddits\n",
    "\n",
    "Explore bow, simple_cnn and ensemble architecture. Visual Evaluation of overfitting.  Test prediction output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "import math   \n",
    "from spacy.util import minibatch , compounding\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from progressbar import ProgressBar, Bar, Percentage\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and targets\n",
    "train_data_full = pd.read_csv(\"train_data.csv\")\n",
    "train_target_full = pd.read_csv(\"train_target.csv\")\n",
    "test_data = pd.read_csv(\"lPunctNumStopLemOovAggTest.csv\") # this is a version of the test_data.csv where i grouped by the \"author\" column and aggregated the subreddits with \" \".join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of the type {\"author1\": \"subreddit1_1 subreddit1_2 subreddit1_3\", \"author2\": \"subreddit2_1 subreddit2_2\", ...} such that subredditI_J != from subredditI_K for all J != K\n",
    "features_dict = {}\n",
    "for author, group in train_data_full.groupby(\"author\"):\n",
    "    features_dict[author] = \" \".join(group[\"subreddit\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AskReddit tall pics StarWars', 'gainit', 'MouseReview MechanicalKeyboards jailbreak AskReddit', 'AskWomen AskMen', 'mildlyinteresting todayilearned']\n"
     ]
    }
   ],
   "source": [
    "print(list(features_dict.values())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary of the kind : {\"author\": [\"author1\", \"author2\", ...], \"subreddit\": [\"subreddit1_1 subreddit1_2 subreddit1_3\",\"subreddit2_1 subreddit2_2\", ... ]}\n",
    "to_be_dfed = {\"author\": list(features_dict.keys()) , \"subreddit\" : list(features_dict.values()) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           author                                          subreddit\n",
      "0          -Jared                       AskReddit tall pics StarWars\n",
      "1         -Peeter                                             gainit\n",
      "2        -evasian  MouseReview MechanicalKeyboards jailbreak AskR...\n",
      "3         -rubiks                                    AskWomen AskMen\n",
      "4  -true_neutral-                    mildlyinteresting todayilearned\n"
     ]
    }
   ],
   "source": [
    "# turn it into a dataframe\n",
    "author_subrdts = pd.DataFrame.from_dict(to_be_dfed)\n",
    "print(author_subrdts.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# create a gender list such that the gender of author_subrdts.loc[i,\"author\"] equuals gender[i]\n",
    "gender = [0 for i in range(len(author_subrdts))]\n",
    "\n",
    "for idx, row in train_target_full.iterrows():\n",
    "    if row.gender == 1:\n",
    "        indexes = author_subrdts.index[author_subrdts[\"author\"] == row.author].tolist()\n",
    "        for i in indexes:\n",
    "                gender[i] += 1\n",
    "\n",
    "if(len(np.unique(gender) == 2)):\n",
    "    print(\"ok\")\n",
    "else:\n",
    "    print(\"there has been an error with gender recognition, please halt the program now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             author                                          subreddit  gender\n",
      "0            -Jared                       AskReddit tall pics StarWars       0\n",
      "1           -Peeter                                             gainit       0\n",
      "2          -evasian  MouseReview MechanicalKeyboards jailbreak AskR...       0\n",
      "3           -rubiks                                    AskWomen AskMen       1\n",
      "4    -true_neutral-                    mildlyinteresting todayilearned       0\n",
      "5           -willis                       tipofmytongue ifyoulikeblank       0\n",
      "6          00708070                                myfriendwantstoknow       0\n",
      "7           0200008                                          AskReddit       1\n",
      "8         05Lanky05                                  trees bestof IAmA       0\n",
      "9   0urlittlesecret  gonewild dirtypenpals AdviceAnimals gifs world...       0\n",
      "10        0utlander  totalwar AskReddit gameofthrones MapPorn Fallo...       0\n",
      "11            0xJRS  Nootropics aww Entrepreneur golf consulting Fi...       0\n",
      "12        105Hummel  TheFalloutDiaries falloutlore tipofmytongue Fa...       0\n",
      "13       11235813__        funny AskReddit swtor secretsanta australia       0\n",
      "14         12013177                            tightdresses funny gifs       0\n"
     ]
    }
   ],
   "source": [
    "# add the column to the data frame\n",
    "author_subrdts[\"gender\"] = gender\n",
    "print(author_subrdts.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_texts) =  4000 len(test_texts) =  1000\n",
      "len(test_texts) == len(test_labels) :  True\n",
      "len(train_data) =  4000 len(test_data) =  1000\n"
     ]
    }
   ],
   "source": [
    "# split data frame in train and validation (notice: train_labels is in the ofrmat spacy requires)\n",
    "split = math.floor(len(author_subrdts)*0.8)\n",
    "\n",
    "seed = 100\n",
    "\n",
    "train_df = author_subrdts.sample(split ,random_state=seed )\n",
    "test_df = author_subrdts.drop(train_df.index)\n",
    "\n",
    "train_texts  = train_df[\"subreddit\"].tolist()\n",
    "test_texts =  test_df[\"subreddit\"].tolist()\n",
    "\n",
    "train_labels = [{'cats': {'1': label == 1,'0': label == 0}} for label in train_df[\"gender\"].tolist()]\n",
    "\n",
    "\n",
    "\n",
    "test_labels = [i for i in test_df[\"gender\"].tolist()]  \n",
    "train_data  = list(zip(train_texts, train_labels))\n",
    "print(\"len(train_texts) = \",len(train_texts),\"len(test_texts) = \" , len(test_texts))\n",
    "print(\"len(test_texts) == len(test_labels) : \", len(test_texts) == len(test_labels) )\n",
    "print(\"len(train_data) = \",len(train_data), \"len(test_data) = \", len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load a learner that speaks english\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create the TextCategorizer with exclusive classes and \"bow\" (bag of words) architecture. The TextCategorizer is the element of the nlp pipe we will be training.\n",
    "if 'textcat' not in nlp.pipe_names:\n",
    "    textcat = nlp.create_pipe(\n",
    "                  \"textcat\",\n",
    "                  config={\"exclusive_classes\": True, \"architecture\": \"ensemble\"  }) #\"bow\"\n",
    "    nlp.add_pipe(textcat)\n",
    "else:\n",
    "    textcat = nlp.get_pipe('textcat')\n",
    "\n",
    "# add labels \n",
    "textcat.add_label(\"1\")\n",
    "textcat.add_label(\"0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will evaluate roc at the end of each epoch of training.\n",
    "def evaluate_roc(nlp,textcat):\n",
    "    print(\"evaluating roc \\n\")\n",
    "    docs = [nlp.tokenizer(tex) for tex in test_texts]\n",
    "    scores , a = textcat.predict(docs)\n",
    "    y_pred = [b[0] for b in scores] \n",
    "    roc = roc_auc_score(test_labels, y_pred)\n",
    "    return roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seeds set\n",
      "strings and lists initialized\n",
      "pipe_exceptions defined\n",
      "other_pipes defined\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  736\n",
      "evaluating roc \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0  losses =  {'textcat': 9.823404482362093} roc =  0.8894467975869593 i =  736\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  736\n",
      "evaluating roc \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  1  losses =  {'textcat': 16.006812849847847} roc =  0.9006135284302401 i =  736\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  736\n",
      "evaluating roc \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  2  losses =  {'textcat': 20.781789980873555} roc =  0.8967988704915928 i =  736\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  736\n",
      "evaluating roc \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  3  losses =  {'textcat': 24.50453746617768} roc =  0.8918084969836992 i =  736\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  736\n",
      "evaluating roc \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  4  losses =  {'textcat': 27.917881497464492} roc =  0.8867873187010653 i =  736\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  736\n",
      "evaluating roc \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  5  losses =  {'textcat': 30.478690824294183} roc =  0.8855294570658452 i =  736\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  736\n",
      "evaluating roc \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  6  losses =  {'textcat': 33.177135079762934} roc =  0.8818123475805415 i =  736\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  736\n",
      "evaluating roc \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  7  losses =  {'textcat': 35.532278382530016} roc =  0.8763855730971634 i =  736\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  736\n",
      "evaluating roc \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  8  losses =  {'textcat': 37.83402553677294} roc =  0.8751841868823 i =  736\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  736\n",
      "evaluating roc \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  9  losses =  {'textcat': 39.859853323127446} roc =  0.8729611089718906 i =  736\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  736\n",
      "evaluating roc \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  10  losses =  {'textcat': 41.88246862717979} roc =  0.8675908099088693 i =  736\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  736\n",
      "evaluating roc \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  11  losses =  {'textcat': 43.92857742387356} roc =  0.8667077396996534 i =  736\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  736\n",
      "evaluating roc \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  12  losses =  {'textcat': 46.01102748613428} roc =  0.8698549608522654 i =  736\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  736\n",
      "evaluating roc \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  13  losses =  {'textcat': 47.972736447593924} roc =  0.8682633808240278 i =  736\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  736\n",
      "evaluating roc \n",
      "\n",
      "epoch =  14  losses =  {'textcat': 49.88400436997952} roc =  0.8702810935695032 i =  736\n",
      "Wall time: 34min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# set required seeds\n",
    "random.seed(1)\n",
    "spacy.util.fix_random_seed(1)\n",
    "\n",
    "\n",
    "\n",
    "# may have to uncomemnt this to avoi warnings\n",
    "#nlp.vocab.vectors.name = 'spacy_pretrained_vectors'\n",
    "\n",
    "print(\"random seeds set\")\n",
    "\n",
    "losses = {}\n",
    "rocs = []\n",
    "\n",
    "print(\"strings and lists initialized\")\n",
    "\n",
    "\n",
    "#learning process\n",
    "pipe_exceptions = ['textcat'] \n",
    "print(\"pipe_exceptions defined\")\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "print(\"other_pipes defined\")\n",
    "with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "    optimizer = nlp.begin_training() # returns an optimizer. I tred to tune it but the default one seems to be the best\n",
    "    for epoch in range(15): # every epoch, all train data are used\n",
    "        print(\"shuffling data...\")\n",
    "        random.shuffle(train_data) # shuffle the data\n",
    "        print(\"creating batches...\")\n",
    "        batches = minibatch(train_data, size=compounding(4., 32., 1.001)) # size of n_th batch is 4*(1.001)^n, up to a maximum of 32\n",
    "        # Iterate through minibatches\n",
    "        pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=737).start() \n",
    "        i = 0\n",
    "        print(\"training...\")\n",
    "        for batch in batches:\n",
    "            # Each batch is a list of (text, label) but we need to\n",
    "            # send separate lists for texts and labels to update().\n",
    "            # This is a quick way to split a list of tuples into lists\n",
    "            texts1, labels = zip(*batch)\n",
    "            nlp.update(texts1, labels, sgd=optimizer, losses=losses, drop = 0.1) # set some drop to avoid overfitting during the first epochs. \n",
    "            i += 1\n",
    "            pbar.update(i)\n",
    "        pbar.finish()\n",
    "        print(\"i = \",  i) # evaluate roc with parameters averaged over epocha\n",
    "        with textcat.model.use_params(optimizer.averages):\n",
    "            rocs.append(evaluate_roc(nlp, textcat))\n",
    "            print( \"epoch = \",epoch,\" losses = \", losses, \"roc = \" , rocs[-1] , \"i = \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [nlp.tokenizer(tex) for tex in test_texts]\n",
    "scores , a = textcat.predict(docs)\n",
    "y_pred = [b[0] for b in scores] \n",
    "roc = roc_auc_score(test_labels, y_pred)\n",
    "\n",
    "df_res = pd.DataFrame({\"author\": test_df[\"author\"].tolist(), \"gender\" : y_pred, \"true_y\" : test_labels  })\n",
    "df_res.to_csv (r'subs_ensemble_e15.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAQGklEQVR4nO3ca4xd11nG8f9DTFoubZ0mkyiyDW6FC42Q2kSjYFSJS11VuaA6HxKUihJTWVhAQKAigaEfuH5IkWhopCpgNaUOAtoQKLHacAlOokJFUiYk5EqJCSEZOcQDSQwlKhB4+XCW24k9zmzPnHMms/z/SUdn77XXmf0uz8wzy+vss1NVSJL68jVrXYAkafwMd0nqkOEuSR0y3CWpQ4a7JHVow1oXAHDOOefU1q1b17oMSVpX7rvvvn+tqpmljr0qwn3r1q3Mzc2tdRmStK4k+eeTHXNZRpI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOvSq+ITqamzd+9mpnu/J6y6f6vkkaSWcuUtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1KFB4Z5kY5Jbk/x9kseSfGeSNya5I8nj7fms1jdJbkhyKMmDSS6a7BAkSccbOnP/CPCnVfVtwNuAx4C9wMGq2gYcbPsAlwLb2mMPcONYK5YkLWvZcE/yeuC7gJsAquq/q+oFYCewv3XbD1zRtncCN9fIPcDGJOePvXJJ0kkNmbm/GVgAfjvJ/Uk+luQbgPOq6hmA9nxu678JeHrR6+db28sk2ZNkLsncwsLCqgYhSXq5IeG+AbgIuLGqLgT+k68uwSwlS7TVCQ1V+6pqtqpmZ2ZmBhUrSRpmSLjPA/NVdW/bv5VR2D97bLmlPR9Z1H/LotdvBg6Pp1xJ0hDLhntV/QvwdJJvbU07gEeBA8Cu1rYLuK1tHwCuaVfNbAeOHlu+kSRNx9Bb/v4E8LtJzgSeAN7P6A/DLUl2A08BV7W+twOXAYeAF1tfSdIUDQr3qnoAmF3i0I4l+hZw7SrrkiStgp9QlaQOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDg8I9yZNJHkryQJK51vbGJHckebw9n9Xak+SGJIeSPJjkokkOQJJ0olOZuX9vVb29qmbb/l7gYFVtAw62fYBLgW3tsQe4cVzFSpKGWc2yzE5gf9veD1yxqP3mGrkH2Jjk/FWcR5J0ioaGewF/nuS+JHta23lV9QxAez63tW8Cnl702vnW9jJJ9iSZSzK3sLCwsuolSUvaMLDfO6rqcJJzgTuS/P0r9M0SbXVCQ9U+YB/A7OzsCcclSSs3aOZeVYfb8xHg08DFwLPHllva85HWfR7Ysujlm4HD4ypYkrS8ZcM9yTcked2xbeDdwMPAAWBX67YLuK1tHwCuaVfNbAeOHlu+kSRNx5BlmfOATyc51v/3qupPk/wNcEuS3cBTwFWt/+3AZcAh4EXg/WOvWpL0ipYN96p6AnjbEu3/BuxYor2Aa8dSnSRpRfyEqiR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUODwz3JGUnuT/KZtv+mJPcmeTzJp5Kc2dpf0/YPteNbJ1O6JOlkTmXm/pPAY4v2PwRcX1XbgOeB3a19N/B8VX0LcH3rJ0maokHhnmQzcDnwsbYf4J3Ara3LfuCKtr2z7dOO72j9JUlTMnTm/hvAzwD/1/bPBl6oqpfa/jywqW1vAp4GaMePtv4vk2RPkrkkcwsLCyssX5K0lGXDPcn3AUeq6r7FzUt0rQHHvtpQta+qZqtqdmZmZlCxkqRhNgzo8w7gPUkuA14LvJ7RTH5jkg1tdr4ZONz6zwNbgPkkG4A3AM+NvXJJ0kktO3Ovqp+rqs1VtRW4Grizqn4AuAu4snXbBdzWtg+0fdrxO6vqhJm7JGlyVnOd+88CH0hyiNGa+k2t/Sbg7Nb+AWDv6kqUJJ2qIcsyX1FVdwN3t+0ngIuX6PNl4Kox1CZJWiE/oSpJHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SerQsuGe5LVJvpDk75I8kuSXWvubktyb5PEkn0pyZmt/Tds/1I5vnewQJEnHGzJz/y/gnVX1NuDtwCVJtgMfAq6vqm3A88Du1n838HxVfQtwfesnSZqiZcO9Rr7Udr+2PQp4J3Bra98PXNG2d7Z92vEdSTK2iiVJyxq05p7kjCQPAEeAO4B/BF6oqpdal3lgU9veBDwN0I4fBc5e4mvuSTKXZG5hYWF1o5AkvcygcK+q/62qtwObgYuBty7VrT0vNUuvExqq9lXVbFXNzszMDK1XkjTAKV0tU1UvAHcD24GNSTa0Q5uBw217HtgC0I6/AXhuHMVKkoYZcrXMTJKNbfvrgHcBjwF3AVe2bruA29r2gbZPO35nVZ0wc5ckTc6G5btwPrA/yRmM/hjcUlWfSfIo8MkkvwrcD9zU+t8E/E6SQ4xm7FdPoG5J0itYNtyr6kHgwiXan2C0/n58+5eBq8ZSnSRpRfyEqiR1yHCXpA4Z7pLUIcNdkjpkuEtShwx3SeqQ4S5JHTLcJalDhrskdchwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdWjbck2xJcleSx5I8kuQnW/sbk9yR5PH2fFZrT5IbkhxK8mCSiyY9CEnSyw2Zub8E/HRVvRXYDlyb5AJgL3CwqrYBB9s+wKXAtvbYA9w49qolSa9o2XCvqmeq6m/b9n8AjwGbgJ3A/tZtP3BF294J3Fwj9wAbk5w/9solSSd1SmvuSbYCFwL3AudV1TMw+gMAnNu6bQKeXvSy+dZ2/Nfak2QuydzCwsKpVy5JOqnB4Z7kG4E/BH6qqv79lbou0VYnNFTtq6rZqpqdmZkZWoYkaYBB4Z7kaxkF++9W1R+15mePLbe05yOtfR7Ysujlm4HD4ylXkjTEkKtlAtwEPFZVH1506ACwq23vAm5b1H5Nu2pmO3D02PKNJGk6Ngzo8w7gB4GHkjzQ2n4euA64Jclu4CngqnbsduAy4BDwIvD+sVYsSVrWsuFeVX/F0uvoADuW6F/AtausS5K0Cn5CVZI6ZLhLUocMd0nqkOEuSR0acrWMJJ02tu797FTP9+R1l0/k6zpzl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1yHCXpA4Z7pLUIcNdkjq0bLgn+XiSI0keXtT2xiR3JHm8PZ/V2pPkhiSHkjyY5KJJFi9JWtqQmfsngEuOa9sLHKyqbcDBtg9wKbCtPfYAN46nTEnSqVg23Kvqc8BzxzXvBPa37f3AFYvab66Re4CNSc4fV7GSpGFWuuZ+XlU9A9Cez23tm4CnF/Wbb20nSLInyVySuYWFhRWWIUlayrjfUM0SbbVUx6raV1WzVTU7MzMz5jIk6fS20nB/9thyS3s+0trngS2L+m0GDq+8PEnSSqw03A8Au9r2LuC2Re3XtKtmtgNHjy3fSJKmZ8NyHZL8PvA9wDlJ5oFfAK4DbkmyG3gKuKp1vx24DDgEvAi8fwI1S5KWsWy4V9V7T3JoxxJ9C7h2tUVJklbHT6hKUocMd0nqkOEuSR0y3CWpQ4a7JHXIcJekDhnuktQhw12SOmS4S1KHDHdJ6pDhLkkdMtwlqUOGuyR1aNm7Qkq92Lr3s1M715PXXT61c0lLceYuSR0y3CWpQy7LvMq5lCBpJZy5S1KHnLmfomnOpCX5O7dSztwlqUOGuyR1yHCXpA655q6vcG1T6ofhLk3AtP9QTvsyVicCr34TCfcklwAfAc4APlZV103iPJJGDFsdb+xr7knOAD4KXApcALw3yQXjPo8k6eQm8YbqxcChqnqiqv4b+CSwcwLnkSSdxCSWZTYBTy/anwe+4/hOSfYAe9rul5J8cYXnOwf41xW+dr1yzKcHx3wayIdWNeZvPtmBSYR7lmirExqq9gH7Vn2yZK6qZlf7ddYTx3x6cMynh0mNeRLLMvPAlkX7m4HDEziPJOkkJhHufwNsS/KmJGcCVwMHJnAeSdJJjH1ZpqpeSvLjwJ8xuhTy41X1yLjPs8iql3bWIcd8enDMp4eJjDlVJyyHS5LWOe8tI0kdMtwlqUPrJtyTXJLki0kOJdm7xPHXJPlUO35vkq3Tr3K8Boz5A0keTfJgkoNJTnrN63qx3JgX9bsySSVZ95fNDRlzku9v3+tHkvzetGsctwE/29+U5K4k97ef78vWos5xSfLxJEeSPHyS40lyQ/v3eDDJRas+aVW96h+M3pj9R+DNwJnA3wEXHNfnx4DfbNtXA59a67qnMObvBb6+bf/o6TDm1u91wOeAe4DZta57Ct/nbcD9wFlt/9y1rnsKY94H/GjbvgB4cq3rXuWYvwu4CHj4JMcvA/6E0eeEtgP3rvac62XmPuSWBjuB/W37VmBHkqU+ULVeLDvmqrqrql5su/cw+kzBejb01hW/Avwa8OVpFjchQ8b8w8BHq+p5gKo6MuUax23ImAt4fdt+A+v8szJV9TnguVfoshO4uUbuATYmOX8151wv4b7ULQ02naxPVb0EHAXOnkp1kzFkzIvtZvSXfz1bdsxJLgS2VNVnplnYBA35Pr8FeEuSzye5p911dT0bMuZfBN6XZB64HfiJ6ZS2Zk71931Z6+V+7kNuaTDotgfryODxJHkfMAt890QrmrxXHHOSrwGuB35oWgVNwZDv8wZGSzPfw+h/Z3+Z5Nur6oUJ1zYpQ8b8XuATVfXrSb4T+J025v+bfHlrYuz5tV5m7kNuafCVPkk2MPqv3Cv9N+jVbtBtHJK8C/gg8J6q+q8p1TYpy435dcC3A3cneZLR2uSBdf6m6tCf7duq6n+q6p+ALzIK+/VqyJh3A7cAVNVfA69ldFOxXo39ti3rJdyH3NLgALCrbV8J3FntnYp1atkxtyWK32IU7Ot9HRaWGXNVHa2qc6pqa1VtZfQ+w3uqam5tyh2LIT/bf8zozXOSnMNomeaJqVY5XkPG/BSwAyDJWxmF+8JUq5yuA8A17aqZ7cDRqnpmVV9xrd9FPoV3my8D/oHRu+wfbG2/zOiXG0bf/D8ADgFfAN681jVPYcx/ATwLPNAeB9a65kmP+bi+d7POr5YZ+H0O8GHgUeAh4Oq1rnkKY74A+DyjK2keAN691jWvcry/DzwD/A+jWfpu4EeAH1n0Pf5o+/d4aBw/195+QJI6tF6WZSRJp8Bwl6QOGe6S1CHDXZI6ZLhLUocMd0nqkOEuSR36f6d5lAJJqG8yAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_pred, bins=\"auto\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's train the model on all train data and predict the test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_data) =  5000\n"
     ]
    }
   ],
   "source": [
    "# prepare all data\n",
    "train_texts  = author_subrdts[\"subreddit\"].tolist()\n",
    "train_labels = [{'cats': {'1': label == 1,'0': label == 0}} for label in author_subrdts[\"gender\"].tolist()]\n",
    "train_data  = list(zip(train_texts, train_labels))\n",
    "print(\"len(train_data) = \", len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new model\n",
    "# load a learner that speaks english\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create the TextCategorizer with exclusive classes and \"bow\" (bag of words) architecture. The TextCategorizer is the element of the nlp pipe we will be training.\n",
    "\n",
    "# link to model architecture description (from spacy documentation): https://spacy.io/api/textcategorizer#architectures\n",
    "if 'textcat' not in nlp.pipe_names:\n",
    "    textcat = nlp.create_pipe(\n",
    "                  \"textcat\", #textcat\n",
    "                  config={\"exclusive_classes\": True, \"architecture\": \"ensemble\"  })\n",
    "    nlp.add_pipe(textcat)\n",
    "else:\n",
    "    textcat = nlp.get_pipe('textcat')\n",
    "\n",
    "# add labels \n",
    "textcat.add_label(\"1\")\n",
    "textcat.add_label(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seeds set\n",
      "strings and lists initialized\n",
      "pipe_exceptions defined\n",
      "other_pipes defined\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "Wall time: 30min 33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# set required seeds\n",
    "random.seed(1)\n",
    "spacy.util.fix_random_seed(1)\n",
    "\n",
    "\n",
    "\n",
    "# may have to uncomemnt this to avoi warnings\n",
    "#nlp.vocab.vectors.name = 'spacy_pretrained_vectors'\n",
    "\n",
    "print(\"random seeds set\")\n",
    "\n",
    "losses = {}\n",
    "rocs = []\n",
    "\n",
    "print(\"strings and lists initialized\")\n",
    "\n",
    "\n",
    "#learning process\n",
    "pipe_exceptions = ['textcat'] \n",
    "print(\"pipe_exceptions defined\")\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "print(\"other_pipes defined\")\n",
    "with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "    optimizer = nlp.begin_training() # returns an optimizer. I tred to tune it but the default one seems to be the best\n",
    "    for epoch in range(15): # every epoch, all train data are used\n",
    "        print(\"shuffling data...\")\n",
    "        random.shuffle(train_data) # shuffle the data\n",
    "        print(\"creating batches...\")\n",
    "        batches = minibatch(train_data, size=compounding(4, 32, 1.001)) # size of n_th batch is 4*(1.001)^n, up to a maximum of 32\n",
    "        # Iterate through minibatches\n",
    "        pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=857).start() \n",
    "        i = 0\n",
    "        print(\"training...\")\n",
    "        for batch in batches:\n",
    "            # Each batch is a list of (text, label) but we need to\n",
    "            # send separate lists for texts and labels to update().\n",
    "            # This is a quick way to split a list of tuples into lists\n",
    "            texts1, labels = zip(*batch)\n",
    "            nlp.update(texts1, labels, sgd=optimizer, losses=losses, drop = 0.1) # set some drop to avoid overfitting during the first epochs. \n",
    "            i += 1\n",
    "            pbar.update(i)\n",
    "        pbar.finish()\n",
    "        print(\"i = \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|############                                                             |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obtained textcat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs done\n",
      "prediction done\n",
      "           author    gender\n",
      "0    --redbeard--  0.000604\n",
      "1       -Allaina-  0.999867\n",
      "2  -AllonsyAlonso  0.002187\n",
      "3          -Beth-  0.216146\n",
      "4        -Greeny-  0.000966\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "textcat = nlp.get_pipe(\"textcat\")\n",
    "print(\"obtained textcat\")\n",
    "# tokenize the texts to pass to the textcat\n",
    "# this line may take a while.\n",
    "docs = [] #nlp.tokenizer(str(tex)) for tex in l\n",
    "texts = test_data[\"subreddit\"].tolist()\n",
    "pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=len(texts)).start()\n",
    "i = 0\n",
    "for tex in texts:\n",
    "    docs.append(nlp.tokenizer(tex))\n",
    "    i += 1\n",
    "    pbar.update(i)\n",
    "pbar.finish()\n",
    "print(\"docs done\")\n",
    "# for each text (sequnece of subreddits separated by a space) the textcat predicts a list [a,b] where a is the probability that the author is a woman, while b is the probablility that the author is a man.\n",
    "# this line may take a while\n",
    "scores, a = textcat.predict(docs)\n",
    "print(\"prediction done\")\n",
    "# so the predictions are the first elements of all lists in scores\n",
    "pred_y = [s[0] for s in scores]\n",
    "# create dataframe for submission\n",
    "subs_test_predictions = pd.DataFrame({\"author\": test_data[\"author\"].tolist(), \"gender\": pred_y})\n",
    "print(subs_test_predictions.head(5))\n",
    "subs_test_predictions.to_csv(r\"subs_test_predictions_ensemble.csv\" , index = False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAPdklEQVR4nO3cbaxlVX3H8e9PRrQ+gs5o7My0F+PYiiaNZIJYE9s6FhAahhfQTFPraCadxFJrrWmL7QsalQT7hJr40KlDOxorUGrKRGwJAYxtU0YGsVSghClQmELl2hmmD8SH0X9fnAW9kPuwr/fec+fM+n6Syd177bXPWf859/7OPmvvs1NVSJL68IzVHoAkaXwMfUnqiKEvSR0x9CWpI4a+JHVkzWoPYD5r166tqamp1R6GJE2U22677ZtVtW62bcd06E9NTbF///7VHoYkTZQk/zbXNqd3JKkjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI8f0N3KXauri6wb3feCyc1dwJJJ0bPBIX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1ZFDoJ3lPkjuTfD3J55I8O8kpSfYluTfJVUlObH2f1dYPtO1TMx7nfa39niRnrUxJkqS5LBj6SdYDvwZsrqrXACcA24APAZdX1SbgMLCj7bIDOFxVrwAub/1Icmrb79XA2cDHk5ywvOVIkuYzdHpnDfBDSdYAzwEeAd4EXNO27wHOb8tb2zpt+5Ykae1XVtW3q+p+4ABw+tJLkCQNtWDoV9W/A38IPMgo7I8AtwGPVdXR1u0gsL4trwceavsebf1fPLN9ln0kSWMwZHrnZEZH6acAPww8F3jLLF3riV3m2DZX+9Ofb2eS/Un2T09PLzQ8SdIiDJneeTNwf1VNV9V3gc8DPwmc1KZ7ADYAD7flg8BGgLb9hcChme2z7POkqtpVVZuravO6det+gJIkSXMZEvoPAmckeU6bm98C3AXcDFzQ+mwHrm3Le9s6bftNVVWtfVu7uucUYBPwleUpQ5I0xJqFOlTVviTXAF8FjgK3A7uA64Ark3ywte1uu+wGPpPkAKMj/G3tce5McjWjN4yjwEVV9b1lrkeSNI8FQx+gqi4BLnla833McvVNVX0LuHCOx7kUuHSRY5QkLRO/kStJHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOjIo9JOclOSaJP+S5O4kr0/yoiQ3JLm3/Ty59U2SjyY5kOSOJKfNeJztrf+9SbavVFGSpNkNPdL/CPC3VfXjwE8AdwMXAzdW1SbgxrYO8BZgU/u3E/gEQJIXAZcArwNOBy554o1CkjQeC4Z+khcAbwR2A1TVd6rqMWArsKd12wOc35a3Ap+ukVuAk5K8DDgLuKGqDlXVYeAG4OxlrUaSNK8hR/ovB6aBP0tye5JPJXku8NKqegSg/XxJ678eeGjG/gdb21ztkqQxGRL6a4DTgE9U1WuB/+X/p3Jmk1naap72p+6c7EyyP8n+6enpAcOTJA01JPQPAgeral9bv4bRm8A32rQN7eejM/pvnLH/BuDhedqfoqp2VdXmqtq8bt26xdQiSVrAgqFfVf8BPJTkx1rTFuAuYC/wxBU424Fr2/Je4G3tKp4zgCNt+ud64MwkJ7cTuGe2NknSmKwZ2O9dwGeTnAjcB7yD0RvG1Ul2AA8CF7a+XwTOAQ4Aj7e+VNWhJB8Abm393l9Vh5alCknSIINCv6q+BmyeZdOWWfoWcNEcj3MFcMViBihJWj5+I1eSOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdWRw6Cc5IcntSb7Q1k9Jsi/JvUmuSnJia39WWz/Qtk/NeIz3tfZ7kpy13MVIkua3mCP9dwN3z1j/EHB5VW0CDgM7WvsO4HBVvQK4vPUjyanANuDVwNnAx5OcsLThS5IWY1DoJ9kAnAt8qq0HeBNwTeuyBzi/LW9t67TtW1r/rcCVVfXtqrofOACcvhxFSJKGGXqk/2Hgt4Dvt/UXA49V1dG2fhBY35bXAw8BtO1HWv8n22fZR5I0BguGfpKfAx6tqttmNs/StRbYNt8+M59vZ5L9SfZPT08vNDxJ0iIMOdJ/A3BekgeAKxlN63wYOCnJmtZnA/BwWz4IbARo218IHJrZPss+T6qqXVW1uao2r1u3btEFSZLmtmDoV9X7qmpDVU0xOhF7U1X9InAzcEHrth24ti3vbeu07TdVVbX2be3qnlOATcBXlq0SSdKC1izcZU6/DVyZ5IPA7cDu1r4b+EySA4yO8LcBVNWdSa4G7gKOAhdV1feW8PySpEVaVOhX1ZeAL7Xl+5jl6puq+hZw4Rz7XwpcuthBSpKWh9/IlaSOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHVkzWoPQJJ6MXXxdYP7PnDZuSsyBo/0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcWDP0kG5PcnOTuJHcmeXdrf1GSG5Lc236e3NqT5KNJDiS5I8lpMx5re+t/b5LtK1eWJGk2Q470jwLvrapXAWcAFyU5FbgYuLGqNgE3tnWAtwCb2r+dwCdg9CYBXAK8DjgduOSJNwpJ0ngsGPpV9UhVfbUt/zdwN7Ae2Arsad32AOe35a3Ap2vkFuCkJC8DzgJuqKpDVXUYuAE4e1mrkSTNa1Fz+kmmgNcC+4CXVtUjMHpjAF7Suq0HHpqx28HWNlf7059jZ5L9SfZPT08vZniSpAUMDv0kzwP+Cvj1qvqv+brO0lbztD+1oWpXVW2uqs3r1q0bOjxJ0gCDQj/JMxkF/mer6vOt+Rtt2ob289HWfhDYOGP3DcDD87RLksZkyNU7AXYDd1fVH8/YtBd44gqc7cC1M9rf1q7iOQM40qZ/rgfOTHJyO4F7ZmuTJI3JkLtsvgH4JeCfk3yttf0OcBlwdZIdwIPAhW3bF4FzgAPA48A7AKrqUJIPALe2fu+vqkPLUoUkaZAFQ7+q/p7Z5+MBtszSv4CL5nisK4ArFjNASdLy8Ru5ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSNrVnsAx4qpi68b3PeBy85dwZFI0srxSF+SOuKRvrrkJzv1yiN9SeqIR/o/gMUcJS6WR5WSVpJH+pLUEY/0jzErNdfsHLY0zEp+kj8WGPoTbKV+ORf7uCv1JnGsjEOT73gP8sUYe+gnORv4CHAC8KmqumzcY9DqWck/Pv+wpYWNNfSTnAB8DPhZ4CBwa5K9VXXXOMeh5XW8h+2xcuJ+pcax2E9Ix/vrfbwb95H+6cCBqroPIMmVwFbA0FeXjoUAPRbGoPEZd+ivBx6asX4QeN3MDkl2Ajvb6v8kuWcJz7cW+OYS9p80vdUL1tyL7mrOh5ZU84/OtWHcoZ9Z2uopK1W7gF3L8mTJ/qravByPNQl6qxesuRfWvHzGfZ3+QWDjjPUNwMNjHoMkdWvcoX8rsCnJKUlOBLYBe8c8Bknq1lind6rqaJJfBa5ndMnmFVV15wo+5bJME02Q3uoFa+6FNS+TVNXCvSRJxwXvvSNJHTH0JakjEx/6Sc5Ock+SA0kunmX7s5Jc1bbvSzI1/lEurwE1/0aSu5LckeTGJHNeszspFqp5Rr8LklSSib+8b0jNSX6+vdZ3JvmLcY9xuQ343f6RJDcnub39fp+zGuNcLkmuSPJokq/PsT1JPtr+P+5IctqSn7SqJvYfo5PB/wq8HDgR+Cfg1Kf1+RXgk215G3DVao97DDX/DPCctvzOHmpu/Z4PfBm4Bdi82uMew+u8CbgdOLmtv2S1xz2GmncB72zLpwIPrPa4l1jzG4HTgK/Psf0c4G8YfcfpDGDfUp9z0o/0n7ytQ1V9B3jitg4zbQX2tOVrgC1JZvuS2KRYsOaqurmqHm+rtzD6PsQkG/I6A3wA+H3gW+Mc3AoZUvMvAx+rqsMAVfXomMe43IbUXMAL2vILmfDv+VTVl4FD83TZCny6Rm4BTkrysqU856SH/my3dVg/V5+qOgocAV48ltGtjCE1z7SD0ZHCJFuw5iSvBTZW1RfGObAVNOR1fiXwyiT/kOSWdgfbSTak5t8D3prkIPBF4F3jGdqqWezf+4Im/X76C97WYWCfSTK4niRvBTYDP7WiI1p589ac5BnA5cDbxzWgMRjyOq9hNMXz04w+zf1dktdU1WMrPLaVMqTmXwD+vKr+KMnrgc+0mr+/8sNbFcueX5N+pD/ktg5P9kmyhtFHwvk+Th3rBt3KIsmbgd8Fzquqb49pbCtloZqfD7wG+FKSBxjNfe6d8JO5Q3+3r62q71bV/cA9jN4EJtWQmncAVwNU1T8Cz2Z0M7bj1bLfumbSQ3/IbR32Atvb8gXATdXOkEyoBWtuUx1/wijwJ32eFxaouaqOVNXaqpqqqilG5zHOq6r9qzPcZTHkd/uvGZ20J8laRtM99411lMtrSM0PAlsAkryKUehPj3WU47UXeFu7iucM4EhVPbKUB5zo6Z2a47YOSd4P7K+qvcBuRh8BDzA6wt+2eiNeuoE1/wHwPOAv2znrB6vqvFUb9BINrPm4MrDm64Ezk9wFfA/4zar6z9Ub9dIMrPm9wJ8meQ+jaY63T/JBXJLPMZqeW9vOU1wCPBOgqj7J6LzFOcAB4HHgHUt+zgn+/5IkLdKkT+9IkhbB0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kd+T8tlyWb7WzxmQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(pred_y, bins=\"auto\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simple_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new model\n",
    "# load a learner that speaks english\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create the TextCategorizer with exclusive classes and \"bow\" (bag of words) architecture. The TextCategorizer is the element of the nlp pipe we will be training.\n",
    "\n",
    "# link to model architecture description (from spacy documentation): https://spacy.io/api/textcategorizer#architectures\n",
    "if 'textcat' not in nlp.pipe_names:\n",
    "    textcat = nlp.create_pipe(\n",
    "                  \"textcat\", #textcat\n",
    "                  config={\"exclusive_classes\": True, \"architecture\": \"simple_cnn\"  })\n",
    "    nlp.add_pipe(textcat)\n",
    "else:\n",
    "    textcat = nlp.get_pipe('textcat')\n",
    "\n",
    "# add labels \n",
    "textcat.add_label(\"1\")\n",
    "textcat.add_label(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seeds set\n",
      "strings and lists initialized\n",
      "pipe_exceptions defined\n",
      "other_pipes defined\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "Wall time: 17min 26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# set required seeds\n",
    "random.seed(1)\n",
    "spacy.util.fix_random_seed(1)\n",
    "\n",
    "\n",
    "\n",
    "# may have to uncomemnt this to avoi warnings\n",
    "#nlp.vocab.vectors.name = 'spacy_pretrained_vectors'\n",
    "\n",
    "print(\"random seeds set\")\n",
    "\n",
    "losses = {}\n",
    "rocs = []\n",
    "\n",
    "print(\"strings and lists initialized\")\n",
    "\n",
    "\n",
    "#learning process\n",
    "pipe_exceptions = ['textcat'] \n",
    "print(\"pipe_exceptions defined\")\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "print(\"other_pipes defined\")\n",
    "with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "    optimizer = nlp.begin_training() # returns an optimizer. I tred to tune it but the default one seems to be the best\n",
    "    for epoch in range(15): # every epoch, all train data are used\n",
    "        print(\"shuffling data...\")\n",
    "        random.shuffle(train_data) # shuffle the data\n",
    "        print(\"creating batches...\")\n",
    "        batches = minibatch(train_data, size=compounding(4, 32, 1.001)) # size of n_th batch is 4*(1.001)^n, up to a maximum of 32\n",
    "        # Iterate through minibatches\n",
    "        pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=857).start() \n",
    "        i = 0\n",
    "        print(\"training...\")\n",
    "        for batch in batches:\n",
    "            # Each batch is a list of (text, label) but we need to\n",
    "            # send separate lists for texts and labels to update().\n",
    "            # This is a quick way to split a list of tuples into lists\n",
    "            texts1, labels = zip(*batch)\n",
    "            nlp.update(texts1, labels, sgd=optimizer, losses=losses, drop = 0.1) # set some drop to avoid overfitting during the first epochs. \n",
    "            i += 1\n",
    "            pbar.update(i)\n",
    "        pbar.finish()\n",
    "        print(\"i = \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|##########                                                               |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obtained textcat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs done\n",
      "prediction done\n",
      "           author        gender\n",
      "0    --redbeard--  3.247646e-08\n",
      "1       -Allaina-  9.972798e-01\n",
      "2  -AllonsyAlonso  1.231719e-02\n",
      "3          -Beth-  1.814199e-01\n",
      "4        -Greeny-  4.576904e-06\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "textcat = nlp.get_pipe(\"textcat\")\n",
    "print(\"obtained textcat\")\n",
    "# tokenize the texts to pass to the textcat\n",
    "# this line may take a while.\n",
    "docs = [] #nlp.tokenizer(str(tex)) for tex in l\n",
    "texts = test_data[\"subreddit\"].tolist()\n",
    "pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=len(texts)).start()\n",
    "i = 0\n",
    "for tex in texts:\n",
    "    docs.append(nlp.tokenizer(tex))\n",
    "    i += 1\n",
    "    pbar.update(i)\n",
    "pbar.finish()\n",
    "print(\"docs done\")\n",
    "# for each text (sequnece of subreddits separated by a space) the textcat predicts a list [a,b] where a is the probability that the author is a woman, while b is the probablility that the author is a man.\n",
    "# this line may take a while\n",
    "scores, a = textcat.predict(docs)\n",
    "print(\"prediction done\")\n",
    "# so the predictions are the first elements of all lists in scores\n",
    "pred_y = [s[0] for s in scores]\n",
    "# create dataframe for submission\n",
    "subs_test_predictions = pd.DataFrame({\"author\": test_data[\"author\"].tolist(), \"gender\": pred_y})\n",
    "print(subs_test_predictions.head(5))\n",
    "subs_test_predictions.to_csv(r\"subs_test_predictions_cnn.csv\" , index = False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(pred_y, bins=\"auto\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new model\n",
    "# load a learner that speaks english\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create the TextCategorizer with exclusive classes and \"bow\" (bag of words) architecture. The TextCategorizer is the element of the nlp pipe we will be training.\n",
    "\n",
    "# link to model architecture description (from spacy documentation): https://spacy.io/api/textcategorizer#architectures\n",
    "if 'textcat' not in nlp.pipe_names:\n",
    "    textcat = nlp.create_pipe(\n",
    "                  \"textcat\", #textcat\n",
    "                  config={\"exclusive_classes\": True, \"architecture\": \"bow\"  })\n",
    "    nlp.add_pipe(textcat)\n",
    "else:\n",
    "    textcat = nlp.get_pipe('textcat')\n",
    "\n",
    "# add labels \n",
    "textcat.add_label(\"1\")\n",
    "textcat.add_label(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seeds set\n",
      "strings and lists initialized\n",
      "pipe_exceptions defined\n",
      "other_pipes defined\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  856\n",
      "Wall time: 5min 3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# set required seeds\n",
    "random.seed(1)\n",
    "spacy.util.fix_random_seed(1)\n",
    "\n",
    "\n",
    "\n",
    "# may have to uncomemnt this to avoi warnings\n",
    "#nlp.vocab.vectors.name = 'spacy_pretrained_vectors'\n",
    "\n",
    "print(\"random seeds set\")\n",
    "\n",
    "losses = {}\n",
    "rocs = []\n",
    "\n",
    "print(\"strings and lists initialized\")\n",
    "\n",
    "\n",
    "#learning process\n",
    "pipe_exceptions = ['textcat'] \n",
    "print(\"pipe_exceptions defined\")\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "print(\"other_pipes defined\")\n",
    "with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "    optimizer = nlp.begin_training() # returns an optimizer. I tred to tune it but the default one seems to be the best\n",
    "    for epoch in range(15): # every epoch, all train data are used\n",
    "        print(\"shuffling data...\")\n",
    "        random.shuffle(train_data) # shuffle the data\n",
    "        print(\"creating batches...\")\n",
    "        batches = minibatch(train_data, size=compounding(4, 32, 1.001)) # size of n_th batch is 4*(1.001)^n, up to a maximum of 32\n",
    "        # Iterate through minibatches\n",
    "        pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=857).start() \n",
    "        i = 0\n",
    "        print(\"training...\")\n",
    "        for batch in batches:\n",
    "            # Each batch is a list of (text, label) but we need to\n",
    "            # send separate lists for texts and labels to update().\n",
    "            # This is a quick way to split a list of tuples into lists\n",
    "            texts1, labels = zip(*batch)\n",
    "            nlp.update(texts1, labels, sgd=optimizer, losses=losses, drop = 0.1) # set some drop to avoid overfitting during the first epochs. \n",
    "            i += 1\n",
    "            pbar.update(i)\n",
    "        pbar.finish()\n",
    "        print(\"i = \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|#########                                                                |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obtained textcat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs done\n",
      "prediction done\n",
      "           author    gender\n",
      "0    --redbeard--  0.228313\n",
      "1       -Allaina-  0.307801\n",
      "2  -AllonsyAlonso  0.090238\n",
      "3          -Beth-  0.277412\n",
      "4        -Greeny-  0.050699\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "textcat = nlp.get_pipe(\"textcat\")\n",
    "print(\"obtained textcat\")\n",
    "# tokenize the texts to pass to the textcat\n",
    "# this line may take a while.\n",
    "docs = [] #nlp.tokenizer(str(tex)) for tex in l\n",
    "texts = test_data[\"subreddit\"].tolist()\n",
    "pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=len(texts)).start()\n",
    "i = 0\n",
    "for tex in texts:\n",
    "    docs.append(nlp.tokenizer(tex))\n",
    "    i += 1\n",
    "    pbar.update(i)\n",
    "pbar.finish()\n",
    "print(\"docs done\")\n",
    "# for each text (sequnece of subreddits separated by a space) the textcat predicts a list [a,b] where a is the probability that the author is a woman, while b is the probablility that the author is a man.\n",
    "# this line may take a while\n",
    "scores, a = textcat.predict(docs)\n",
    "print(\"prediction done\")\n",
    "# so the predictions are the first elements of all lists in scores\n",
    "pred_y = [s[0] for s in scores]\n",
    "# create dataframe for submission\n",
    "subs_test_predictions = pd.DataFrame({\"author\": test_data[\"author\"].tolist(), \"gender\": pred_y})\n",
    "print(subs_test_predictions.head(5))\n",
    "subs_test_predictions.to_csv(r\"subs_test_predictions_bow.csv\" , index = False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(pred_y, bins=\"auto\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that while during validation we have good rocs, the roc on test is very low. But in class we saw that given the validation set is big enough and no heuristic is used during train-validation splitting, then also generalization should be good.\n",
    "Notice: the actual model I use is more complex, but it presents the same issue, in that it has roc as high as roc = 97 during validation, and then drops to 89 during prediction.\n",
    "Is there something about spacy i am actually missing?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myEnv]",
   "language": "python",
   "name": "conda-env-myEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

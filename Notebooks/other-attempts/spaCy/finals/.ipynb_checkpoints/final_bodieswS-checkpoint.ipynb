{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q:\\anaconda\\envs\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "Q:\\anaconda\\envs\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "Q:\\anaconda\\envs\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "Q:\\anaconda\\envs\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "Q:\\anaconda\\envs\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "Q:\\anaconda\\envs\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sys\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import math   \n",
    "from spacy.util import minibatch , compounding, decaying\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from progressbar import ProgressBar, Bar, Percentage\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy_transformers.util import cyclic_triangular_rate\n",
    "from thinc.neural.optimizers import Adam\n",
    "from thinc.neural import Model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing:  lPunctNumStopLemOovAgg.csv\n",
      "           author                                          subreddit  \\\n",
      "0          -Jared                       starwars askreddit pics tall   \n",
      "1         -Peeter                                             gainit   \n",
      "2        -evasian  mousereview askreddit mechanicalkeyboards jail...   \n",
      "3         -rubiks                                    askwomen askmen   \n",
      "4  -true_neutral-                    todayilearned mildlyinteresting   \n",
      "\n",
      "                                                body  \n",
      "0  neil diamond sweet caroline chiropractor go re...  \n",
      "1                                           read faq  \n",
      "2  receive deathadder black edition yesterday try...  \n",
      "3  best monkey d haight floyd great gig bank like...  \n",
      "4  gt urban ear different pair month different pr...  \n",
      "len(train_data_full) before elimitating empties =  5000 train_data_full.loc[0,'body'] =  starwars askreddit pics tall neil diamond sweet caroline chiropractor go religiously age extreme low pain run track cross country important remember somewhat slow process worth result oh not worry not crack adjustment look like pattern envelope middle school photo come bad motivator want spell time beat hangover like bacon better taylor ham egg easy toast hash browns think mean taylor ham not matter damn delicious\n",
      "len(train_data_full) after elimitating empties =  5000\n",
      "creating gender...\n",
      "len(gender) =  5000\n",
      "gender created. Adding subreddits...\n",
      "populating gender list...\n",
      "gender list polulated. Creating n_words column for future use...\n",
      "split =  1349\n",
      "len(train_texts) =  2698\n",
      "len(train_data) =  2698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seeds set\n",
      "strings and lists initialized\n",
      "pipe_exceptions defined\n",
      "other_pipes defined\n",
      "data shuffled\n",
      "batches created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  555\n",
      "data shuffled\n",
      "batches created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  555\n",
      "data shuffled\n",
      "batches created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  555\n",
      "data shuffled\n",
      "batches created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  555\n",
      "data shuffled\n",
      "batches created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  555\n",
      "data shuffled\n",
      "batches created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  555\n",
      "data shuffled\n",
      "batches created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  555\n",
      "data shuffled\n",
      "batches created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  555\n",
      "data shuffled\n",
      "batches created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  555\n",
      "data shuffled\n",
      "batches created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  555\n",
      "data shuffled\n",
      "batches created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  555\n",
      "data shuffled\n",
      "batches created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  555\n",
      "data shuffled\n",
      "batches created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  555\n",
      "data shuffled\n",
      "batches created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  555\n",
      "data shuffled\n",
      "batches created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  555\n",
      "Wall time: 46min 56s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "train_target_full = pd.read_csv(\"train_target.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# this part enables lPunctAgg lemmatization.\n",
    "path = r\"csv\"\n",
    "files = listdir(path)\n",
    "files = [f for f in files if \"Agg\" in f and not \".npy\" in f]\n",
    "files = [\"lPunctNumStopLemOovAgg.csv\"]\n",
    "for f in files:\n",
    "    print(\"doing: \", f)\n",
    "    #lemmed = lPunctAgg\n",
    "    train_data_full = pd.read_csv(path+\"\\\\\"+f)\n",
    "    print(train_data_full.head(5))\n",
    "    train_data_full[\"body\"] = train_data_full[\"subreddit\"]+\" \"+train_data_full[\"body\"]\n",
    "    # print(\"dropping subreddits and utc\")\n",
    "    train_data_full.drop([\"subreddit\"], axis = 1, inplace = True)\n",
    "    print(\"len(train_data_full) before elimitating empties = \",len(train_data_full), \"train_data_full.loc[0,'body'] = \", train_data_full.loc[0,'body'])\n",
    "    # enable following line to exclude rows with less than 4 words\n",
    "    #train_data_full = train_data_full[train_data_full.body.count(\" \") >3]\n",
    "    #if f == \"lPunctNumAgg.csv\":\n",
    "    train_data_full.replace(to_replace = \"empty\", value = \"\", inplace = True)\n",
    "    #train_data_full = train_data_full[train_data_full.body != \"empty\"]\n",
    "    #train_data_full = train_data_full.dropna()\n",
    "    train_data_full[\"body\"].fillna(\"\",inplace = True)\n",
    "    train_data_full.reset_index(drop  = True, inplace = True)\n",
    "    print(\"len(train_data_full) after elimitating empties = \",len(train_data_full))\n",
    "    print(\"creating gender...\")\n",
    "\n",
    "    ###### end of the part to be commented to avoid lPunctAgg lemmatization\n",
    "\n",
    "\n",
    "    # common part to all procedures. This should never be commented\n",
    "\n",
    "    gender = [0 for i in range(len(train_data_full))]\n",
    "    print(\"len(gender) = \", len(gender))\n",
    "    # for idx, row in train_data_full.iterrows():\n",
    "    #     gender.append(train_target_full[train_target_full[\"author\"] == row.author].iloc[0,1])\n",
    "    print(\"gender created. Adding subreddits...\")\n",
    "\n",
    "\n",
    "    print(\"populating gender list...\")\n",
    "    for idx, row in train_target_full.iterrows():\n",
    "        if row.gender == 1:\n",
    "            #print(\"inside th if\")\n",
    "            indexes = train_data_full.index[train_data_full[\"author\"] == row.author].tolist()\n",
    "            for i in indexes:\n",
    "                #print(\"inside the for\")\n",
    "                gender[i] += 1\n",
    "    print(\"gender list polulated. Creating n_words column for future use...\")\n",
    "    #print(fake_gender)\n",
    "    if(len(np.unique(gender) == 2)):\n",
    "        train_data_full[\"gender\"] = gender\n",
    "    else:\n",
    "        print(\"there has been an error with gender recognition, please halt the program now\")\n",
    "\n",
    "    ### end of the common part\n",
    "\n",
    "    # useless part\n",
    "\n",
    "    #print(\"gender list polulated. Creating n_words column for future use...\")\n",
    "    #train_data_full_sg = train_data_full.sort_values([\"gender\"])\n",
    "    #train_data_full_sg.reset_index(inplace = True)\n",
    "\n",
    "    #train_data_full[\"n_words\"] = [tex.count(\" \") for tex in train_data_full[\"body\"].tolist()]\n",
    "    # #rand = random.shuffle([True for t in range(split)]+[False for u in range(len(train))])\n",
    "\n",
    "\n",
    "    # end of useless part\n",
    "\n",
    "    # this is the balanced part\n",
    "\n",
    "    train_data_full_m = train_data_full.loc[train_data_full[\"gender\"] == 0, :]\n",
    "    train_data_full_f = train_data_full.loc[train_data_full[\"gender\"] == 1, :]\n",
    "\n",
    "    split = math.floor(len(train_data_full_f)*1)\n",
    "    \n",
    "    print(\"split = \",split)\n",
    "\n",
    "\n",
    "    seed = 100\n",
    "    train_data_sample_m =  train_data_full_m.sample(n = split, random_state = seed)\n",
    "    train_texts_m =train_data_sample_m[\"body\"].tolist()\n",
    "    #test_data_sample_m = train_data_full_m.drop(train_data_sample_m.index)\n",
    "    #test_texts_m = test_data_sample_m[\"body\"].tolist()\n",
    "\n",
    "    train_data_sample_f =  train_data_full_f.sample(n = split, random_state = seed)\n",
    "    train_texts_f = train_data_sample_f[\"body\"].tolist()\n",
    "    #test_data_sample_f = train_data_full_f.drop(train_data_sample_f.index)\n",
    "    #test_texts_f = test_data_sample_f[\"body\"].tolist()\n",
    "\n",
    "    train_texts = train_texts_m + train_texts_f\n",
    "    #test_texts = test_texts_m + test_texts_f\n",
    "\n",
    "    train_labels = [{'cats': {'1': False, '0': True}} for i in range(split)] + [{'cats': {'1': True, '0': False}} for i in range(split)]\n",
    "    #test_labels = [0 for i in range(len(train_data_full_m)-split)] + [1 for i in range(len(train_data_full_f)-split)]\n",
    "\n",
    "    #train_labels_reg = [0 for i in range(split)] + [1 for j in range(split)]\n",
    "\n",
    "    #train_texts , train_labels_reg = shuffle(train_texts , train_labels_reg , random_state = 0)\n",
    "\n",
    "    # end of balanced part\n",
    "\n",
    "    # unbalanced part\n",
    "\n",
    "#     seed = 100\n",
    "#     split = math.floor(len(train_data_full)*0.8)\n",
    "#     print(\"split = \",split)\n",
    "\n",
    "    #train_df = train_data_full\n",
    "    #test_df = train_data_full.drop(train_df.index)\n",
    "\n",
    "    #train_texts  = train_df[\"body\"].tolist()\n",
    "    #test_texts =  test_df[\"body\"].tolist()\n",
    "\n",
    "#     train_labels = [{'cats': {'1': label == 1,'0': label == 0}} for label in train_df[\"gender\"].tolist()]\n",
    "    #test_labels = [i for i in test_df[\"gender\"].tolist()]  #[0 for i in range(len(train_data_full_m)-split)] + [1 for i in range(len(train_data_full_f)-split)]\n",
    "\n",
    "#     # end of unbalanced part\n",
    "\n",
    "\n",
    "#     # common part to all approaches : do not comment\n",
    "\n",
    "    print(\"len(train_texts) = \",len(train_texts))\n",
    "\n",
    "\n",
    "\n",
    "    train_data  = list(zip(train_texts, train_labels))\n",
    "    #print(\"len(test_texts) == len(test_labels) : \", len(test_texts) == len(test_labels) )\n",
    "    print(\"len(train_data) = \",len(train_data))\n",
    "\n",
    "    # end of common part to all approaches\n",
    "\n",
    "\n",
    "    # carica un learner che parla inglese\n",
    "    nlp = spacy.blank(\"en\")\n",
    "\n",
    "    # Create the TextCategorizer with exclusive classes and \"bow\" (bag of words) architecture.\n",
    "    if 'textcat' not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\n",
    "                      \"textcat\",\n",
    "                      config={\"exclusive_classes\": True, \"architecture\": \"ensemble\"}) #https://github.com/explosion/spaCy/issues/3611 #, \"n_gram_size\" : 2, \"attr\": \"lower\"  #\"softmax_last_hidden\"\n",
    "        nlp.add_pipe(textcat)\n",
    "    else:\n",
    "        textcat = nlp.get_pipe('textcat')\n",
    "\n",
    "    # create labels (do they coomute)\n",
    "    textcat.add_label(\"1\")\n",
    "    textcat.add_label(\"0\")\n",
    "\n",
    "\n",
    "    # add the textcat to the spacy pipe\n",
    "\n",
    "    #nlp.add_pipe(textcat)\n",
    "\n",
    "\n",
    "    # %%time\n",
    "\n",
    "    random.seed(1)\n",
    "    spacy.util.fix_random_seed(1)\n",
    "\n",
    "    # write it otherwise it is not happy\n",
    "    nlp.vocab.vectors.name = 'spacy_pretrained_vectors'\n",
    "\n",
    "    # speaks for itself\n",
    "    print(\"random seeds set\")\n",
    "    name = f.replace(\".csv\",\"\")\n",
    "    losses = {}\n",
    "    rocs = []\n",
    "    run_title = \"bow_bal_\"+name\n",
    "    output = \"\"\n",
    "\n",
    "    print(\"strings and lists initialized\")\n",
    "    dec = decaying(0.2 , 0.0, 2e-4)\n",
    "\n",
    "    #learning process\n",
    "    #for batch_size in [10, 50, 100, 200, 300] : \n",
    "    pipe_exceptions = ['textcat'] #(\"trf_wordpiecer\", \"trf_tok2vec) are required, #https://github.com/explosion/spaCy/blob/master/examples/training/train_textcat.py\n",
    "    print(\"pipe_exceptions defined\")\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    print(\"other_pipes defined\")\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "        optimizer = nlp.begin_training()\n",
    "#         batch_size  = 4\n",
    "#         learn_rate = 0.001 #0.0005\n",
    "#         learn_rates = cyclic_triangular_rate(learn_rate / 3, learn_rate * 3, 2 * len(train_data) // batch_size)\n",
    "#         nlp.begin_training()\n",
    "#         ops = Model.ops\n",
    "#         beta1 = 0.9 #0.9\n",
    "#         beta2 = 0.999 #0.999\n",
    "#         eps = 1e-10 #1e-8\n",
    "#         L2 = 0.0 #1e-6\n",
    "#         max_grad_norm = 1 #1.0\n",
    "#         optimizer = Adam(ops, learn_rate, L2=L2, beta1=beta1, beta2=beta2, eps=eps)\n",
    "#         optimizer.max_grad_norm = max_grad_norm\n",
    "#         optimizer.device = ops.device\n",
    "        for epoch in range(15):\n",
    "            random.shuffle(train_data)\n",
    "            print(\"data shuffled\")\n",
    "            # Create the batch generator with batch size = 8\n",
    "            #optimizer.learn_rate = float(next(learn_rates))\n",
    "            batches = minibatch(train_data, size = compounding(4., 32., 1.001) )  #batch_size\n",
    "            print(\"batches created\")\n",
    "            # Iterate through minibatches\n",
    "            pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=556).start()  #8387 is the total number of iterations needed for an unbalanced 0.8 trainset. 0.04 of unbalanced would be approx 1540 . 4595 for lemmatized balanced. 8420 for lemmatized unbalanced\n",
    "            i = 0                                                                    # 737 for lPunctAgg and lPunctNumAgg unbalanced, 466 for balanced\n",
    "            for batch in batches:\n",
    "                # Each batch is a list of (text, label) but we need to\n",
    "                # send separate lists for texts and labels to update().\n",
    "                # This is a quick way to split a list of tuples into lists\n",
    "                texts1, labels = zip(*batch)\n",
    "                # print(\"updating\")\n",
    "                #print(\"len(texts1) = \",len(texts1), \"len(labels) = \",len(labels), \"texts1 =\", texts1, \"labels = \", labels  )\n",
    "                nlp.update(texts1, labels, sgd=optimizer, losses=losses, drop = next(dec))\n",
    "                i += 1\n",
    "                pbar.update(i)\n",
    "            pbar.finish()\n",
    "            print(\"i = \",  i)\n",
    "#                 rocs.append(evaluate_roc(nlp, textcat))\n",
    "#                 output += f\"    epoch = {epoch}, losses = {losses}, roc = {rocs[-1]} \\n \"\n",
    "#                 print( \"epoch = \",epoch,\" losses = \", losses, \"roc = \" , rocs[-1] , \"i = \", i)\n",
    "    \n",
    "    \n",
    "#     docs = [nlp.tokenizer(tex) for tex in test_texts]\n",
    "#     #textcat = nlp.get_pipe(\"textcat\")\n",
    "#     scores , a = textcat.predict(docs) \n",
    "#     pred_y= [b[0] for b in scores] #[1 if b[0] > b[1] else 0 for b in scores]\n",
    "#     roc = roc_auc_score(test_labels, y_pred)\n",
    "#     print(roc)\n",
    "    \n",
    "\n",
    "\n",
    "#     df_res = pd.DataFrame({\"author\": test_df[\"author\"].tolist(), \"pred_y\" : y_pred, \"true_y\" : test_labels  })\n",
    "#     df_res.to_csv (r'Q:\\tooBigToDrive\\data-mining\\kaggle\\my_models\\spaCy\\results\\finals\\csv\\bodies.csv', index = False, header=True)\n",
    "#     with textcat.model.use_params(optimizer.averages):\n",
    "#         nlp.to_disk(r'Q:\\tooBigToDrive\\data-mining\\kaggle\\my_models\\spaCy\\savedModels\\bodieswS_bal')  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded nlp\n",
      "obtained textcat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded df\n",
      "<class 'pandas.core.series.Series'> 15000\n",
      "15000 insightfulquestions glasgow youshouldknow iama askreddit britishproblems get master degree exactly unexpected positive come lose hearing glass half hi brownish pubis bright blonde eyebrow be sure relevant hear earn £ happy go k k year notice huge change life style afford nice thing happy not drastic change clearly millionaire buy nice car huge house not happy person k accord theory say shite go make million comparatively measly ki suppose relative make comment kind redundant oh look play pie pint think oran mor big converted church west end near botanic garden week fortnight pie pint beer whilst watch local play £ token black man group friend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs done 15000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "[[1.0557201e-04 9.9989438e-01]]\n",
      "0.00010557201\n",
      "           author    pred_y\n",
      "0    --redbeard--  0.000106\n",
      "1       -Allaina-  0.800658\n",
      "2  -AllonsyAlonso  0.117949\n",
      "3          -Beth-  0.600263\n",
      "4        -Greeny-  0.938785\n"
     ]
    }
   ],
   "source": [
    "#nlp = spacy.load(r'Q:\\tooBigToDrive\\data-mining\\kaggle\\my_models\\spaCy\\savedModels\\bodieswS_bal')\n",
    "print(\"loaded nlp\")\n",
    "textcat = nlp.get_pipe(\"textcat\")\n",
    "print(\"obtained textcat\")\n",
    "df = pd.read_csv(r\"Q:\\tooBigToDrive\\data-mining\\kaggle\\my_models\\spaCy\\results\\finals\\csv\\test\\\\\"+\"lPunctNumStopLemOovAggTest.csv\")\n",
    "print(\"loaded df\")\n",
    "series = df[\"subreddit\"]+\" \"+df[\"body\"] \n",
    "print(type(series), len(series))\n",
    "l = series.values\n",
    "print(len(l), l[0])\n",
    "docs = [] #nlp.tokenizer(str(tex)) for tex in l\n",
    "pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=len(l)).start()\n",
    "i = 0\n",
    "for tex in l:\n",
    "    docs.append(nlp.tokenizer(str(tex)))\n",
    "    i += 1\n",
    "    pbar.update(i)\n",
    "pbar.finish()\n",
    "print(\"docs done\", len(docs))\n",
    "pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=len(docs)).start()\n",
    "# scores, a = textcat.predict(docs)\n",
    "i = 0\n",
    "scores = []\n",
    "for doc in docs:\n",
    "    scores.append(textcat.predict([doc])[0])\n",
    "    i += 1\n",
    "    pbar.update(i)\n",
    "pbar.finish()\n",
    "print(\"done\")\n",
    "print(scores[0])\n",
    "pred_y = [v[0][0] for v in scores]\n",
    "print(pred_y[0])\n",
    "bodieswS_test_predictions = pd.DataFrame({\"author\": df[\"author\"].tolist(), \"pred_y\": pred_y})\n",
    "print(bodieswS_test_predictions.head(5))\n",
    "\n",
    "\n",
    "\n",
    "bodieswS_test_predictions.to_csv(r\"Q:\\tooBigToDrive\\data-mining\\kaggle\\my_models\\spaCy\\results\\finals\\csv\\test\\bodieswS_bal_test_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "name = \"ensemble_balanced_e15_wS\"\n",
    "plt.hist(pred_y, bins=\"auto\")\n",
    "plt.title(name)\n",
    "plt.savefig(r\"\\images\\\\\"+name+\".png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myEnv]",
   "language": "python",
   "name": "conda-env-myEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Challange: *Reddit Gender Text-Classification*\n",
    "\n",
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Numpy & matplotlib for notebooks \n",
    "%pylab inline\n",
    "\n",
    "# Pandas \n",
    "import pandas as pd # Data analysis and manipulation \n",
    "\n",
    "# Sklearn \n",
    "from sklearn import utils\n",
    "from sklearn.preprocessing import StandardScaler # to standardize features by removing the mean and scaling to unit variance (z=(x-u)/s)\n",
    "from sklearn.neural_network import MLPClassifier # Multi-layer Perceptron classifier which optimizes the log-loss function using LBFGS or sdg.\n",
    "from sklearn.model_selection import train_test_split # to split arrays or matrices into random train and test subsets\n",
    "from sklearn.model_selection import KFold # K-Folds cross-validator providing train/test indices to split data in train/test sets.\n",
    "from sklearn.decomposition import PCA, TruncatedSVD # Principal component analysis (PCA); dimensionality reduction using truncated SVD.\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import MultinomialNB # Naive Bayes classifier for multinomial models\n",
    "from sklearn.feature_extraction.text import CountVectorizer # Convert a collection of text documents to a matrix of token counts\n",
    "from sklearn.metrics import roc_auc_score as roc # Compute Area Under the Receiver Operating Characteristic Curve from prediction scores\n",
    "from sklearn.metrics import roc_curve, auc # Compute ROC; Compute Area Under the Curve (AUC) using the trapezoidal rule\n",
    "\n",
    "# Matplotlib\n",
    "import matplotlib # Data visualization\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.patches as mpatches  \n",
    "\n",
    "# Seaborn\n",
    "import seaborn as sns # Statistical data visualization (based on matplotlib)\n",
    "\n",
    "# Tqdm \n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "# Gensim \n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "# Regular Expressions\n",
    "import re # String manipulation\n",
    "\n",
    "# Nltk\n",
    "import nltk # lemmatization\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag  \n",
    "from nltk.corpus import wordnet as wn  \n",
    "from nltk.stem.snowball import SnowballStemmer # stemmer\n",
    "\n",
    "from bs4 import BeautifulSoup   \n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_data = pd.read_csv(\"../input/dataset/train_data.csv\")\n",
    "target = pd.read_csv(\"../input/dataset/train_target.csv\")\n",
    "test_data = pd.read_csv(\"../input/dataset/test_data.csv\")\n",
    "\n",
    "# Create author's gender dictionary\n",
    "author_gender = {}\n",
    "for i in range(len(target)):\n",
    "    author_gender[target.author[i]] = target.gender[i]\n",
    "    \n",
    "# X is the list of aggregated comments   \n",
    "X = []\n",
    "\n",
    "# y is the list of genders\n",
    "y = []\n",
    "\n",
    "# Populate the dictionary with keys (\"authors\") and values (\"gender\")\n",
    "for author, group in train_data.groupby(\"author\"):\n",
    "    X.append(group.body.str.cat(sep = \" \"))\n",
    "    y.append(author_gender[author])\n",
    "\n",
    "# Same thing with test dataset\n",
    "X_test = []\n",
    "authors_test = []\n",
    "for author, group in test_data.groupby(\"author\"):\n",
    "    X_test.append(group.body.str.cat(sep = \" \"))\n",
    "    authors_test.append(author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing, Optimize Input for doc2vec Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pre-processing functions\n",
    "def remove_number(text):\n",
    "    num = re.compile(r'[-+]?[.\\d]*[\\d]+[:,.\\d]*')\n",
    "    return num.sub(r'NUMBER', text)\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'URL',text)\n",
    "\n",
    "def remove_repeat_punct(text):\n",
    "    rep = re.compile(r'([!?.]){2,}')\n",
    "    return rep.sub(r'\\1 REPEAT', text)\n",
    "\n",
    "def remove_elongated_words(text):\n",
    "    rep = re.compile(r'\\b(\\S*?)([a-z])\\2{2,}\\b')\n",
    "    return rep.sub(r'\\1\\2 ELONG', text)\n",
    "\n",
    "def remove_allcaps(text):\n",
    "    caps = re.compile(r'([^a-z0-9()<>\\'`\\-]){2,}')\n",
    "    return caps.sub(r'ALLCAPS', text)\n",
    "\n",
    "def transcription_smile(text):\n",
    "    eyes = \"[8:=;]\"\n",
    "    nose = \"['`\\-]\"\n",
    "    smiley = re.compile(r'[8:=;][\\'\\-]?[)dDp]')\n",
    "    #smiley = re.compile(r'#{eyes}#{nose}[)d]+|[)d]+#{nose}#{eyes}/i')\n",
    "    return smiley.sub(r'SMILE', text)\n",
    "\n",
    "def transcription_sad(text):\n",
    "    eyes = \"[8:=;]\"\n",
    "    nose = \"['`\\-]\"\n",
    "    smiley = re.compile(r'[8:=;][\\'\\-]?[(\\\\/]')\n",
    "    return smiley.sub(r'SADFACE', text)\n",
    "\n",
    "def transcription_heart(text):\n",
    "    heart = re.compile(r'<3')\n",
    "    return heart.sub(r'HEART', text)\n",
    "\n",
    "# Tags Part of Speech (POS), because the lemmatizer requires it\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "# Create lemmatizer\n",
    "word_Lemmatized = WordNetLemmatizer()\n",
    "\n",
    "def review_to_words1(raw_body):\n",
    "    # remove html tags\n",
    "    body_text = BeautifulSoup(raw_body).get_text() \n",
    "    #letters_only = re.sub(\"[^a-zA-Z]\", \" \", body_text) \n",
    "    # lowercase all text\n",
    "    words = body_text.lower()\n",
    "    # remove urls\n",
    "    text = remove_URL(words)\n",
    "    # remove numbers\n",
    "    text = remove_number(text)\n",
    "    # remove smiles\n",
    "    text = transcription_sad(text)\n",
    "    text = transcription_smile(text)\n",
    "    text = transcription_heart(text)\n",
    "    text = remove_elongated_words(text)\n",
    "    words = remove_repeat_punct(text)\n",
    "    # tokenizes and pass to lemmatizer, which lemmatizes taking tags into account (see before)\n",
    "    words = word_tokenize(words)\n",
    "    # we don't remove stop words, because doing it on combination with removing the 40 (trial & error estimated parameter) most utilized words (see below) decreases performance\n",
    "    #stops = set(stopwords.words(\"english\"))                  \n",
    "    #meaningful_words = [w for w in words if not w in stops]\n",
    "    Final_words = []\n",
    "    for word, tag in pos_tag(words):\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    #if len(Final_words)<11: return -1\n",
    "    # returns lemmatized texts as strings \n",
    "    return( \" \".join(Final_words))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/bs4/__init__.py:389: UserWarning: \"http://mishkanyc.bandcamp.com/album/the-swoup-serengeti\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/conda/lib/python3.6/site-packages/bs4/__init__.py:389: UserWarning: \"http://instagram.com/samerkhouzami\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/conda/lib/python3.6/site-packages/bs4/__init__.py:389: UserWarning: \"http://e24.no/bil/her-leverer-amazon-bil-i-pappeske/22708631\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/conda/lib/python3.6/site-packages/bs4/__init__.py:314: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/opt/conda/lib/python3.6/site-packages/bs4/__init__.py:389: UserWarning: \"http://i.imgur.com/BH8hH63.png\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/opt/conda/lib/python3.6/site-packages/bs4/__init__.py:389: UserWarning: \"http://imgur.com/s8XW68r\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "clean_train_comments = [review_to_words1(x) for x in X]\n",
    "clean_comments_test = [review_to_words1(x) for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function formats the input dor doc2vec\n",
    "def label_sentences(corpus, label_type):\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(TaggedDocument(v.split(), [label]))\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create doc2vec input\n",
    "X_train = label_sentences(clean_train_comments, 'Train')\n",
    "X_test = label_sentences(clean_comments_test, 'Test')\n",
    "all_data = X_train + X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `doc2vec`: Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:00<00:00, 1082750.31it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1277524.33it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1186205.49it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1188440.60it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1336574.36it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1210826.79it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1377055.34it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1275465.34it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1296679.39it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1333981.30it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1310126.35it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1289246.00it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1360770.85it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1350605.06it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1229082.06it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1330996.91it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1280351.66it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 997314.06it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1003854.29it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1266319.67it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1288731.03it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1216110.41it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1261957.19it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1210774.36it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1259891.26it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1238481.69it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1305193.32it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1193971.93it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1189283.05it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1175220.72it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 1199983.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "# window: qhow many neighboring words should the moel look at\n",
    "# negative :som words are negatively weighted\n",
    "# min_count: once-appearing words are discarded\n",
    "model_dbow = Doc2Vec(dm=1, vector_size=400, window=7, negative=5, min_count=1, alpha=0.065)\n",
    "# creates the vocabulary. tdqm is the progress bar\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])\n",
    "# trianing. The sub doc2vec is trained on training and test set\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns vectorized aggragated texts\n",
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors\n",
    "    \n",
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 400, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 400, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier: Model Definition and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.51712106\n",
      "Validation score: 0.824000\n",
      "Iteration 2, loss = 0.33308173\n",
      "Validation score: 0.856000\n",
      "Iteration 3, loss = 0.29067624\n",
      "Validation score: 0.856000\n",
      "Iteration 4, loss = 0.26098065\n",
      "Validation score: 0.854000\n",
      "Iteration 5, loss = 0.23977749\n",
      "Validation score: 0.846000\n",
      "Iteration 6, loss = 0.22093390\n",
      "Validation score: 0.842000\n",
      "Iteration 7, loss = 0.20381316\n",
      "Validation score: 0.838000\n",
      "Iteration 8, loss = 0.18785099\n",
      "Validation score: 0.842000\n",
      "Iteration 9, loss = 0.17403692\n",
      "Validation score: 0.844000\n",
      "Iteration 10, loss = 0.16122524\n",
      "Validation score: 0.838000\n",
      "Iteration 11, loss = 0.14877849\n",
      "Validation score: 0.844000\n",
      "Iteration 12, loss = 0.13735856\n",
      "Validation score: 0.848000\n",
      "Iteration 13, loss = 0.12766286\n",
      "Validation score: 0.838000\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0005, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='invscaling',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=400,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=0, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=True,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpClf = MLPClassifier(solver = 'adam', activation= 'relu' ,alpha = 0.0005, verbose = True, early_stopping = True,\n",
    "                         learning_rate = 'invscaling', max_iter = 400, random_state = 0)\n",
    "\n",
    "\n",
    "# Final fit\n",
    "mlpClf.fit(train_vectors_dbow, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test and save output\n",
    "y_score = mlpClf.predict_proba(test_vectors_dbow)[:,1]\n",
    "np.save(\"../working/y_testD2V\",y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "We laso tried an XGB regressor, but these predictions will perform worse when submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "my_model1 = XGBRegressor(objective = \"reg:logistic\",n_estimators=2100, learning_rate=0.01, n_jobs=4,subsample = 0.9,\n",
    "                       min_child_weight = 1,max_depth=4,gamma=1.5,colsample_bytree=0.6,random_state=0)\n",
    "my_model1.fit(train_vectors_dbow, y)\n",
    "y_scoreX = my_model1.predict(test_vectors_dbow)\n",
    "np.save(\"../working/y_testD2VX\",y_scoreX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

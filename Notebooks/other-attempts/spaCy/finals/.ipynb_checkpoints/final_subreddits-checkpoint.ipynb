{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q:\\anaconda\\envs\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "Q:\\anaconda\\envs\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "Q:\\anaconda\\envs\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "Q:\\anaconda\\envs\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "Q:\\anaconda\\envs\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "Q:\\anaconda\\envs\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "#from spacy.matcher import PhraseMatcher\n",
    "import math   \n",
    "from spacy.util import minibatch , compounding\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from progressbar import ProgressBar, Bar, Percentage\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy_transformers.util import cyclic_triangular_rate\n",
    "from thinc.neural.optimizers import Adam\n",
    "from thinc.neural import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_full = pd.read_csv(\"train_data.csv\")\n",
    "train_target_full = pd.read_csv(\"train_target.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "features_dict = {}\n",
    "for author, group in train_data_full.groupby(\"author\"):\n",
    "    features_dict[author] = \" \".join(group[\"subreddit\"].unique())\n",
    "print(len(features_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           author                                          subreddit\n",
      "0          -Jared                       AskReddit tall pics StarWars\n",
      "1         -Peeter                                             gainit\n",
      "2        -evasian  MouseReview MechanicalKeyboards jailbreak AskR...\n",
      "3         -rubiks                                    AskWomen AskMen\n",
      "4  -true_neutral-                    mildlyinteresting todayilearned 5000\n"
     ]
    }
   ],
   "source": [
    "to_be_dfed = {\"author\": list(features_dict.keys()) , \"subreddit\" : list(features_dict.values()) }\n",
    "author_subrdts = pd.DataFrame.from_dict(to_be_dfed)\n",
    "print(author_subrdts.head(5), len(author_subrdts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "len(gender) =  5000\n"
     ]
    }
   ],
   "source": [
    "gender = [0 for i in range(len(author_subrdts))]\n",
    "\n",
    "for idx, row in train_target_full.iterrows():\n",
    "    if row.gender == 1:\n",
    "        indexes = author_subrdts.index[author_subrdts[\"author\"] == row.author].tolist()\n",
    "        for i in indexes:\n",
    "                #print(\"inside the for\")\n",
    "                gender[i] += 1\n",
    "\n",
    "if(len(np.unique(gender) == 2)):\n",
    "    print(\"ok\")\n",
    "else:\n",
    "    print(\"there has been an error with gender recognition, please halt the program now\")\n",
    "\n",
    "print(\"len(gender) = \", len(gender))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             author                                          subreddit  gender\n",
      "0            -Jared                       AskReddit tall pics StarWars       0\n",
      "1           -Peeter                                             gainit       0\n",
      "2          -evasian  MouseReview MechanicalKeyboards jailbreak AskR...       0\n",
      "3           -rubiks                                    AskWomen AskMen       1\n",
      "4    -true_neutral-                    mildlyinteresting todayilearned       0\n",
      "5           -willis                       tipofmytongue ifyoulikeblank       0\n",
      "6          00708070                                myfriendwantstoknow       0\n",
      "7           0200008                                          AskReddit       1\n",
      "8         05Lanky05                                  trees bestof IAmA       0\n",
      "9   0urlittlesecret  gonewild dirtypenpals AdviceAnimals gifs world...       0\n",
      "10        0utlander  totalwar AskReddit gameofthrones MapPorn Fallo...       0\n",
      "11            0xJRS  Nootropics aww Entrepreneur golf consulting Fi...       0\n",
      "12        105Hummel  TheFalloutDiaries falloutlore tipofmytongue Fa...       0\n",
      "13       11235813__        funny AskReddit swtor secretsanta australia       0\n",
      "14         12013177                            tightdresses funny gifs       0\n"
     ]
    }
   ],
   "source": [
    "author_subrdts[\"gender\"] = gender\n",
    "print(author_subrdts.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(author_f) =  1349\n",
      "len(train_data) =  2698\n"
     ]
    }
   ],
   "source": [
    "# balanced\n",
    "author_f = author_subrdts[author_subrdts[\"gender\"] == 1]\n",
    "author_m = author_subrdts[author_subrdts[\"gender\"] == 0]\n",
    "print(\"len(author_f) = \", len(author_f))\n",
    "seed = 100\n",
    "split = math.floor(len(author_f)*1)\n",
    "\n",
    "train_f = author_f.sample(n = split, random_state = seed)\n",
    "#test_f = author_f.drop(train_f.index)\n",
    "\n",
    "train_m = author_m.sample(n  = split, random_state = seed)\n",
    "#test_m = author_m.drop(train_m.index)\n",
    "\n",
    "train_subs_f = train_f[\"subreddit\"].tolist()\n",
    "train_subs_m = train_m[\"subreddit\"].tolist()\n",
    "#test_sub_f = test_f[\"subreddit\"].tolist()\n",
    "#test_subs_m = test_m[\"subreddit\"].tolist()\n",
    "\n",
    "train_texts = train_subs_f + train_subs_m\n",
    "train_labels = [{'cats': {'1': True ,'0': False}} for label in range(split)] + [{'cats': {'1': False ,'0': True }} for label in range(split)]\n",
    "#test_texts = test_subs_f + test_subs_m\n",
    "#test_labels = [1 for i in range(split)] + [0 for i in range(split)]\n",
    "\n",
    "train_data = list(zip(train_texts, train_labels))\n",
    "print(\"len(train_data) = \", len(train_data))\n",
    "\n",
    "# end of balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unbalanced\n",
    "# train_texts  = author_subrdts[\"subreddits\"].tolist()\n",
    "# train_labels = [{'cats': {'1': label == 1,'0': label == 0}} for label in author_subrdts[\"gender\"].tolist()]\n",
    "# train_data  = list(zip(train_texts, train_labels))\n",
    "# print(\"len(train_data) = \", len(train_data))\n",
    "\n",
    "# end unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# carica un learner che parla inglese\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create the TextCategorizer with exclusive classes and \"bow\" (bag of words) architecture.\n",
    "if 'textcat' not in nlp.pipe_names:\n",
    "    textcat = nlp.create_pipe(\n",
    "                  \"textcat\",\n",
    "                  config={\"exclusive_classes\": True, \"architecture\": \"bow\" }) #https://github.com/explosion/spaCy/issues/3611 #, \"n_gram_size\" : 2, \"attr\": \"lower\" \n",
    "    nlp.add_pipe(textcat)\n",
    "else:\n",
    "    textcat = nlp.get_pipe('textcat')\n",
    "\n",
    "# create labels (do they coomute)\n",
    "textcat.add_label(\"1\")\n",
    "textcat.add_label(\"0\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# add the textcat to the spacy pipe\n",
    "\n",
    "#nlp.add_pipe(textcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seeds set\n",
      "strings and lists initialized\n",
      "pipe_exceptions defined\n",
      "other_pipes defined\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  555\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n",
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  555\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  555\n",
      "Wall time: 43.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "random.seed(1)\n",
    "spacy.util.fix_random_seed(1)\n",
    "\n",
    "# train_data for unique suvredidts, trian_data_nu for not unique, train_data_nuau for not unique in train and unique in test\n",
    "train_data = train_data\n",
    "# write it otherwise it is not happy\n",
    "#nlp.vocab.vectors.name = 'spacy_pretrained_vectors'\n",
    "\n",
    "# speaks for itself\n",
    "print(\"random seeds set\")\n",
    "\n",
    "losses = {}\n",
    "rocs = []\n",
    "run_title = \"RUN : spacy Ensemble Lemmatized Averaged drop = 0.1 , compound(500,1000,1.001), ngram_size = 2  \\n\"\n",
    "output = \"\"\n",
    "\n",
    "print(\"strings and lists initialized\")\n",
    "\n",
    "f_w = sum([1 if y == 1 else 0 for y in  author_subrdts[\"gender\"].tolist()])\n",
    "m_w = sum([1 if y == 0 else 0 for y in  author_subrdts[\"gender\"].tolist()])\n",
    "\n",
    "#learning process\n",
    "#for batch_size in [10, 50, 100, 200, 300] : \n",
    "pipe_exceptions = ['textcat'] #( ,\"trf_wordpiecer\", \"trf_tok2vec\" ) are required, #https://github.com/explosion/spaCy/blob/master/examples/training/train_textcat.py\n",
    "print(\"pipe_exceptions defined\")\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "print(\"other_pipes defined\")\n",
    "with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "    optimizer = nlp.begin_training(component_cfg = {\"textcat\":{\"weights\": [1./f_w, 1./m_w]}})\n",
    "#     batch_size  = 4\n",
    "#     learn_rate = 0.0005\n",
    "#     learn_rates = cyclic_triangular_rate(learn_rate / 3, learn_rate * 3, 2 * len(train_data) // batch_size)\n",
    "#     nlp.begin_training()\n",
    "#     ops = Model.ops\n",
    "#     beta1 = 0.9 #0.9\n",
    "#     beta2 = 0.999 #0.999\n",
    "#     eps = 1e-10 #1e-8\n",
    "#     L2 = 0.0 #1e-6\n",
    "#     max_grad_norm = 1 #1.0\n",
    "#     optimizer = Adam(ops, learn_rate, L2=L2, beta1=beta1, beta2=beta2, eps=eps)\n",
    "#     optimizer.max_grad_norm = max_grad_norm\n",
    "#     optimizer.device = ops.device\n",
    "    for epoch in range(3):\n",
    "        print(\"shuffling data...\")\n",
    "        random.shuffle(train_data)\n",
    "        #optimizer.learn_rate = float(next(learn_rates))\n",
    "        # Create the batch generator with batch size = 8\n",
    "        print(\"creating batches...\")\n",
    "        batches = minibatch(train_data, size=compounding(4, 32, 1.001)) # compounding(500., 3000., 1.001)  compounding(4, 32, 1.001)\n",
    "        # Iterate through minibatches\n",
    "        pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=2000).start()  #8387 is the total number of iterations needed for an unbalanced 0.8 trainset. 0.04 of unbalanced would be approx 1540 . 4595 for lemmatized balanced. 8420 for lemmatized unbalanced\n",
    "        i = 0\n",
    "        print(\"training...\")\n",
    "        for batch in batches:\n",
    "            # Each batch is a list of (text, label) but we need to\n",
    "            # send separate lists for texts and labels to update().\n",
    "            # This is a quick way to split a list of tuples into lists\n",
    "            texts1, labels = zip(*batch)\n",
    "            nlp.update(texts1, labels, sgd=optimizer, losses=losses, drop = 0.1) #, drop = 0.2\n",
    "            i += 1\n",
    "            pbar.update(i)\n",
    "        pbar.finish()\n",
    "        print(\"i = \",  i)\n",
    "#         with textcat.model.use_params(optimizer.averages):\n",
    "#             rocs.append(evaluate_roc(nlp, textcat))\n",
    "#             output += f\"    epoch = {epoch}, losses = {losses}, roc = {rocs[-1]} \\n \"\n",
    "#             print( \"epoch = \",epoch,\" losses = \", losses, \"roc = \" , rocs[-1] , \"i = \", i)\n",
    "            \n",
    "# sys.stdout = open(\"spacyEnsembleLemmatizedAveraged_output_sys.txt\", \"a\")\n",
    "# print(output)\n",
    "# sys.stdout.close()\n",
    "        \n",
    "# with open(\"spacyEnsembleLemmatizedAveraged_output_file.txt\", \"a\") as f:\n",
    "#     f.write(run_title + output)\n",
    "#     f.close()\n",
    "\n",
    "# # save the model\n",
    "# output_dir = %pwd\n",
    "# nlp.to_disk(\"ensembleLA\")\n",
    "# print(\"Saved model to\", output_dir)\n",
    "\n",
    "#load it\n",
    "# print(\"Loading from\", output_dir)\n",
    "#nlp2 = spacy.load(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    }
   ],
   "source": [
    "with textcat.model.use_params(optimizer.averages):\n",
    "    nlp.to_disk(r'Q:\\tooBigToDrive\\data-mining\\kaggle\\my_models\\spaCy\\savedModels\\subs_bal')\n",
    "    print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded nlp\n",
      "obtained textcat\n",
      "loaded df\n",
      "docs done\n",
      "done\n",
      "[0.48052195 0.519478  ]\n",
      "[0.48052195, 0.51025856, 0.2669244, 0.40504095, 0.43180564, 0.63537675, 0.48134655, 0.3874973, 0.22327794, 0.007003122, 0.042660423, 0.649002, 0.5144257, 0.2682138, 0.43009955]\n",
      "           author    pred_y\n",
      "0    --redbeard--  0.480522\n",
      "1       -Allaina-  0.510259\n",
      "2  -AllonsyAlonso  0.266924\n",
      "3          -Beth-  0.405041\n",
      "4        -Greeny-  0.431806\n"
     ]
    }
   ],
   "source": [
    "# # to be continued with prediction part\n",
    "# import spacy\n",
    "# import pandas as pd\n",
    "\n",
    "nlp = spacy.load(r'ubs_bal')\n",
    "print(\"loaded nlp\")\n",
    "textcat = nlp.get_pipe(\"textcat\")\n",
    "print(\"obtained textcat\")\n",
    "df = pd.read_csv(\"lPunctNumStopLemOovAggTest.csv\")\n",
    "print(\"loaded df\")\n",
    "docs = [nlp.tokenizer(tex) for tex in df[\"subreddit\"].tolist()]\n",
    "print(\"docs done\")\n",
    "scores, a = textcat.predict(docs)\n",
    "print(\"done\")\n",
    "print(scores[0])\n",
    "pred_y = [s[0] for s in scores]\n",
    "print(pred_y[0:15])\n",
    "subs_test_predictions = pd.DataFrame({\"author\": df[\"author\"].tolist(), \"gender\": pred_y})\n",
    "print(subs_test_predictions.head(5))\n",
    "subs_test_predictions.to_csv(r\"subs_bal_test_predictions\" , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights = [1./f_w, 1./m_w]\n",
    "#[0.37892216, 0.39194405, 0.18831716, 0.32629508, 0.31537768, 0.45214972, 0.41050243, 0.26181972, 0.19685002, 0.017932957, 0.05777456, 0.4633669, 0.36835364, 0.24319068, 0.30869114]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nothing\n",
    "#[0.37892216, 0.39194405, 0.18831716, 0.32629508, 0.31537768, 0.45214972, 0.41050243, 0.26181972, 0.19685002, 0.017932957, 0.05777456, 0.4633669, 0.36835364, 0.24319068, 0.30869114]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myEnv]",
   "language": "python",
   "name": "conda-env-myEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

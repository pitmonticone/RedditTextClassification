{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Challange: *Reddit Gender Text-Classification* \n",
    "\n",
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Numpy\n",
    "import numpy as np\n",
    "\n",
    "#Sklearn\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV # Exhaustive search over specified parameter values for a given estimator\n",
    "from sklearn.model_selection import cross_val_score # Evaluate a score by cross-validation\n",
    "from sklearn.model_selection import KFold # K-Folds cross-validator providing train/test indices to split data in train/test sets.\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score # Compute Area Under the Receiver Operating Characteristic Curve from prediction scores\n",
    "from sklearn.feature_extraction.text import CountVectorizer # Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "#XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Matplotlib\n",
    "import matplotlib # Data visualization\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.patches as mpatches \n",
    "\n",
    "#Pickle\n",
    "import pickle # To load files\n",
    "\n",
    "# Joblib\n",
    "import joblib # To save models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed data to save tine\n",
    "with open(\"../input/challengedadata/comments.txt\", \"rb\") as f:\n",
    "    clean_train_comments = pickle.load(f) \n",
    "    f.close()\n",
    "\n",
    "with open(\"../input/challengedadata/targets.txt\", \"rb\") as ft:\n",
    "    y = pickle.load(ft) \n",
    "    ft.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                             max_features = 2000, ngram_range=(1, 2)) \n",
    "# converts in np array\n",
    "train_data_features = vectorizer.fit_transform(clean_train_comments).toarray()\n",
    "\n",
    "# create vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "# counts how many times a word appears\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# removes the 40 most utilized words\n",
    "for _ in range(40):\n",
    "    index = np.argmax(dist)\n",
    "    train_data_features = np.delete(train_data_features, index, axis = 1)\n",
    "    \n",
    "X_len = [[len(x)] for x in train_data_features] \n",
    "s = np.concatenate((train_data_features,np.array(X_len)),axis = 1)\n",
    "\n",
    "# 5000 rows (one per author),  and 2000-40+1 (X_len) features\n",
    "s.shape\n",
    "\n",
    "y = np.array(y)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"learning_rate\":[0.03,0.05,0.07,0.01,0.15,0.2,0.25,0.3],'min_child_weight': [1,4,5,8],'gamma': [0.0, 0.1,0.2, 0.3,0.4,0.5,0.6,0.8],\n",
    "               'subsample': [0.6,0.7,0.8,0.9,1], 'colsample_bytree': [0.3,0.4,0.5, 0.6,0.7,0.8,0.9,1],\n",
    "               'max_depth': [2,3,4,5,6,7,8,10,12,15], 'scale_pos_weight': [1,2.70, 10, 25, 50, 75, 100, 1000] }\n",
    "\n",
    "parameters0 = {'min_child_weight': [1,8],'gamma': [0.6,0.8],\n",
    "               'subsample': [0.9], 'colsample_bytree': [0.6],\n",
    "               'max_depth': [4], 'scale_pos_weight': [1,2.70, 10, 25, 50, 75, 100, 1000] }\n",
    "\n",
    "    \n",
    "xgb = XGBRegressor(objective = \"reg:logistic\", n_estimators=10000, \n",
    "                 tree_method = \"gpu_hist\", gpu_id = 0)\n",
    "\n",
    "\n",
    "# Model exploration\n",
    "xgbClf = GridSearchCV(xgb, param_grid = parameters0, cv = StratifiedKFold(n_splits=10, shuffle = True, random_state = 1001), scoring = \"roc_auc\" ,verbose=True, n_jobs=-1)\n",
    "\n",
    "# Model fit\n",
    "xgbClf.fit(s, y, verbose=False)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(xgbClf, '../working/xgbClf.pkl')\n",
    "\n",
    "print(\"xgbCLf.best_score = \", xgbClf.best_score_)\n",
    "print(\"xgbCLf.best_estimator_ = \", xgbClf.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Fitting 10 folds for each of 32 candidates, totalling 320 fits\n",
    "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
    "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed: 24.1min\n",
    "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed: 105.6min\n",
    "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed: 172.3min finished\n",
    "xgbCLf.best_score =  0.8425215483825477\n",
    "xgbCLf.best_estimator_ =  XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1,\n",
    "             colsample_bynode=1, colsample_bytree=0.6, gamma=0.8, gpu_id=0,\n",
    "             importance_type='gain', interaction_constraints=None,\n",
    "             learning_rate=0.300000012, max_delta_step=0, max_depth=4,\n",
    "             min_child_weight=1, missing=nan, monotone_constraints=None,\n",
    "             n_estimators=10000, n_jobs=0, num_parallel_tree=1,\n",
    "             objective='reg:logistic', random_state=0, reg_alpha=0,\n",
    "             reg_lambda=1, scale_pos_weight=1, subsample=0.9,\n",
    "             tree_method='gpu_hist', validate_parameters=False, verbosity=None)\n",
    "CPU times: user 1min, sys: 14.5 s, total: 1min 14s\n",
    "Wall time: 2h 53min 29s\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

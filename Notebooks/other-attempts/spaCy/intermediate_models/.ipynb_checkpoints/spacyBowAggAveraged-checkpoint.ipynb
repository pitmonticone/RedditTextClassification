{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#####run only once (the first time)\n",
    "#!pip install --upgrade spacy #incompatible with (!pip install spacy-pytorch-transformers)\n",
    "#!pip install torch===1.4.0 torchvision===0.5.0 -f https://download.pytorch.org/whl/torch_stable.html  #this is useful only on jupyter\n",
    "#!pip uninstall spacy-pytorch-transformers #this fixes \n",
    "#!pip install spacy-transformers  # keep it commentedon kaggle\n",
    "#!pip install spacy-pytorch-transformers #it is deprecated https://github.com/explosion/spaCy/issues/4382\n",
    "#!python -m spacy download en_trf_bertbaseuncased_lg  #on kaggle, uncomment only this! \n",
    "#####\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import math   \n",
    "from spacy.util import minibatch , compounding, decaying\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from progressbar import ProgressBar, Bar, Percentage\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy_transformers.util import cyclic_triangular_rate\n",
    "from thinc.neural.optimizers import Adam\n",
    "from thinc.neural import Model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# uncomment this to enable gpu (not recommended: 8gb of graphic memeory is not enogh)\n",
    "\n",
    "# import thinc_gpu_ops\n",
    "# print(thinc_gpu_ops.AVAILABLE) \n",
    "# is_using_gpu = spacy.prefer_gpu()\n",
    "# if is_using_gpu:\n",
    "#     torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "#     print(\"done\")\n",
    "\n",
    "#end of gpu part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lPunctAgg.csv', 'lPunctNumAgg.csv', 'lPunctNumLemAgg.csv', 'lPunctNumLemOovAgg.csv', 'lPunctNumOovAgg.csv', 'lPunctNumPersAgg.csv', 'lPunctNumPersLemAgg.csv', 'lPunctNumPersLemOovAgg.csv', 'lPunctNumStopLemAgg.csv', 'lPunctNumStopLemOovAgg.csv', 'lPunctNumStopLemOovAlphaAgg.csv', 'lPunctNumStopLemOovAlphaSentAgg.csv', 'lPunctNumStopOovAgg.csv', 'lPunctNumStopOovAlphaSentAgg.csv']\n"
     ]
    }
   ],
   "source": [
    "files = listdir(r\"csv\") # path to all lemmatizations\n",
    "files = [f for f in files if \"Agg\" in f]\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_roc(nlp,textcat):\n",
    "    print(\"evaluating roc \\n\")\n",
    "    docs = [nlp.tokenizer(tex) for tex in test_texts]\n",
    "    scores , a = textcat.predict(docs) \n",
    "    y_pred = [b[0] for b in scores]\n",
    "    roc = roc_auc_score(test_labels, y_pred)\n",
    "    return roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_roc1(nlp, textcat):\n",
    "    print(\"evaluate1\")\n",
    "    docs = [nlp(tex) for tex in test_texts]\n",
    "\n",
    "    print(\"evaluate2\")\n",
    "    y_pred = [doc.cats[\"1\"] for doc in docs]\n",
    "    #y_pred = [b[0]  for b in scores]\n",
    "    print(\"evaluate3\")\n",
    "    roc = roc_auc_score(test_labels, y_pred)\n",
    "    print(\"evaluate4\")\n",
    "    return roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_target_full = pd.read_csv(\"Q:\\\\tooBigToDrive\\data-mining\\kaggle\\data\\csv\\\\train_target.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path = r\"Q:\\tooBigToDrive\\data-mining\\kaggle\\data\\csv\"\n",
    "files = listdir(path)\n",
    "files = [f for f in files if \"Agg\" in f and not \".npy\" in f]\n",
    "files = [\"lPunctNumStopLemOovAlphaAgg.csv\"]\n",
    "for f in files:\n",
    "    print(\"doing: \", f)\n",
    "    train_data_full = pd.read_csv(path+\"\\\\\"+f)\n",
    "    print(train_data_full.head(5))\n",
    "    #train_data_full[\"body\"] = train_data_full[\"subreddit\"]+\" \"+train_data_full[\"body\"]\n",
    "    # print(\"dropping subreddits\")\n",
    "    #train_data_full.drop([\"subreddit\"], axis = 1, inplace = True)\n",
    "    print(\"len(train_data_full) before elimitating empties = \",len(train_data_full), \"train_data_full.loc[0,'body'] = \", train_data_full.loc[0,'body'])\n",
    "    # enable following line to exclude rows with less than 4 words\n",
    "    #train_data_full = train_data_full[train_data_full.body.str.count(\" \") >3]\n",
    "    train_data_full.replace(to_replace = \"empty\", value = \"\", inplace = True)\n",
    "    train_data_full[\"body\"].fillna(\"\",inplace = True)\n",
    "    train_data_full.reset_index(drop  = True, inplace = True)\n",
    "    print(\"len(train_data_full) after elimitating empties = \",len(train_data_full))\n",
    "    print(\"creating gender...\")\n",
    "\n",
    "    ###### end of the part to be commented to avoid lPunctAgg lemmatization\n",
    "\n",
    "\n",
    "    # common part to all procedures. This should never be commented\n",
    "\n",
    "    gender = [0 for i in range(len(train_data_full))]\n",
    "    print(\"len(gender) = \", len(gender))\n",
    "    print(\"gender created. Adding subreddits...\")\n",
    "\n",
    "\n",
    "    print(\"populating gender list...\")\n",
    "    for idx, row in train_target_full.iterrows():\n",
    "        if row.gender == 1:\n",
    "            indexes = train_data_full.index[train_data_full[\"author\"] == row.author].tolist()\n",
    "            for i in indexes:\n",
    "                gender[i] += 1\n",
    "    print(\"gender list polulated. Creating n_words column for future use...\")\n",
    "    if(len(np.unique(gender) == 2)):\n",
    "        train_data_full[\"gender\"] = gender\n",
    "    else:\n",
    "        print(\"there has been an error with gender recognition, please halt the program now\")\n",
    "\n",
    "    ### end of the common part\n",
    "\n",
    "    # this is the balanced part\n",
    "\n",
    "#     train_data_full_m = train_data_full.loc[train_data_full[\"gender\"] == 0, :]\n",
    "#     train_data_full_f = train_data_full.loc[train_data_full[\"gender\"] == 1, :]\n",
    "\n",
    "#     split = math.floor(len(train_data_full_f)*0.8)\n",
    "#     print(\"split = \",split)\n",
    "\n",
    "#     seed  = 100\n",
    "\n",
    "#     train_data_sample_m =  train_data_full_m.sample(n = split, random_state = seed)\n",
    "#     train_texts_m =train_data_sample_m[\"body\"].tolist()\n",
    "#     test_data_sample_m = train_data_full_m.drop(train_data_sample_m.index)\n",
    "#     #test_data_sample_m = test_data_sample_m.reset_index() \n",
    "#     test_texts_m = test_data_sample_m[\"body\"].tolist()\n",
    "\n",
    "#     train_data_sample_f =  train_data_full_f.sample(n = split, random_state = seed)\n",
    "#     train_texts_f = train_data_sample_f[\"body\"].tolist()\n",
    "#     test_data_sample_f = train_data_full_f.drop(train_data_sample_f.index)\n",
    "#     #test_data_sample_f = test_data_sample_f.reset_index() \n",
    "#     test_texts_f = test_data_sample_f[\"body\"].tolist()\n",
    "\n",
    "#     train_texts = train_texts_m + train_texts_f\n",
    "#     test_texts = test_texts_m + test_texts_f\n",
    "\n",
    "#     train_labels = [{'cats': {'1': False, '0': True}} for i in range(split)] + [{'cats': {'1': True, '0': False}} for i in range(split)]\n",
    "#     test_labels = [0 for i in range(len(train_data_full_m)-split)] + [1 for i in range(len(train_data_full_f)-split)]\n",
    "\n",
    "#     train_labels_reg = [0 for i in range(split)].extend([1 for j in range(split)])\n",
    "#     #test_texts, test_labels = shuffle(test_texts , test_labels , random_state = 0)\n",
    "#     #train_texts , train_labels_reg = shuffle(train_texts , train_labels_reg , random_state = 0)\n",
    "\n",
    "    # end of balanced part\n",
    "\n",
    "    # unbalanced part\n",
    "\n",
    "    seed = 100\n",
    "    split = math.floor(len(train_data_full)*0.8)\n",
    "    print(\"split = \",split)\n",
    "\n",
    "    train_df = train_data_full.sample(split, random_state = seed)\n",
    "    #train_df = train_df[train_df.body.str.count(\" \") > 3]\n",
    "    \n",
    "    test_df = train_data_full.drop(train_df.index)\n",
    "\n",
    "    \n",
    "    print(\"len(train_df) before removing non bigrams = \",len(train_df))\n",
    "    train_df = train_df[train_df.body.str.count(\" \") >= 1 ]\n",
    "    print(min([t.count(\" \") for t in train_df.body.tolist()]))\n",
    "    print(\"len(train_df) after removing non bigrams = \",len(train_df))\n",
    "    train_texts  = train_df[\"body\"].tolist()\n",
    "    test_texts =  test_df[\"body\"].tolist()\n",
    "\n",
    "    train_labels = [{'cats': {'1': label == 1,'0': label == 0}} for label in train_df[\"gender\"].tolist()]\n",
    "    test_labels = [i for i in test_df[\"gender\"].tolist()]  #[0 for i in range(len(train_data_full_m)-split)] + [1 for i in range(len(train_data_full_f)-split)]\n",
    "\n",
    "    # end of unbalanced part\n",
    "\n",
    "\n",
    "#     # common part to all approaches : do not comment\n",
    "\n",
    "    print(\"len(train_texts) = \",len(train_texts),\"len(test_texts) = \" , len(test_texts))\n",
    "\n",
    "\n",
    "\n",
    "    train_data  = list(zip(train_texts, train_labels))\n",
    "    print(\"len(test_texts) == len(test_labels) : \", len(test_texts) == len(test_labels) )\n",
    "    print(\"len(train_data) = \",len(train_data), \"len(test_data) = \", len(test_texts))\n",
    "\n",
    "    # end of common part to all approaches\n",
    "\n",
    "    # load a lerner\n",
    "    nlp = spacy.blank(\"en\")\n",
    "\n",
    "    if 'textcat' not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\n",
    "                      \"textcat\",\n",
    "                      config={\"exclusive_classes\": True, \"architecture\": \"ensemble\"}) #https://github.com/explosion/spaCy/issues/3611\n",
    "        nlp.add_pipe(textcat, last = True)\n",
    "    else:\n",
    "        textcat = nlp.get_pipe('textcat')\n",
    "    #nlp.add_pipe(nlp.create_pipe(\"sentencizer\"))\n",
    "    # creaye labels\n",
    "    textcat.add_label(\"1\")\n",
    "    textcat.add_label(\"0\")\n",
    "\n",
    "\n",
    "    random.seed(1)\n",
    "    spacy.util.fix_random_seed(1)\n",
    "\n",
    "    # give name to vectors (unnecessary but avoids warning)\n",
    "    nlp.vocab.vectors.name = 'spacy_pretrained_vectors'\n",
    "\n",
    "    print(\"random seeds set\")\n",
    "    name = f.replace(\".csv\",\"\")\n",
    "    losses = {}\n",
    "    rocs = []\n",
    "    run_title = \"bow_bal_\"+name\n",
    "    output = \"\"\n",
    "\n",
    "    print(\"strings and lists initialized\")\n",
    "    dec = decaying(0.6 , 0.2, 1e-4)\n",
    "\n",
    "    #learning process\n",
    "    pipe_exceptions = ['textcat'] #https://github.com/explosion/spaCy/blob/master/examples/training/train_textcat.py\n",
    "    print(\"pipe_exceptions defined\")\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    print(\"other_pipes defined\")\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "        optimizer = nlp.begin_training()\n",
    "        # uncomment this to set your own parameters (found taht default ones perform the best, as reported in the spaCy recipe: https://spacy.io/usage/training#tips)\n",
    "#         batch_size  = 4\n",
    "#         learn_rate = 0.001 #0.0005\n",
    "#         learn_rates = cyclic_triangular_rate(learn_rate / 3, learn_rate * 3, 2 * len(train_data) // batch_size)\n",
    "#         nlp.begin_training()\n",
    "#         ops = Model.ops\n",
    "#         beta1 = 0.9 #0.9\n",
    "#         beta2 = 0.999 #0.999\n",
    "#         eps = 1e-10 #1e-8\n",
    "#         L2 = 0.0 #1e-6\n",
    "#         max_grad_norm = 1 #1.0\n",
    "#         optimizer = Adam(ops, learn_rate, L2=L2, beta1=beta1, beta2=beta2, eps=eps)\n",
    "#         optimizer.max_grad_norm = max_grad_norm\n",
    "#         optimizer.device = ops.device\n",
    "        for epoch in range(4):\n",
    "            random.shuffle(train_data)\n",
    "            print(\"data shuffled\")\n",
    "            # Create the batch generator with batch size = 8\n",
    "            #optimizer.learn_rate = float(next(learn_rates))\n",
    "            batches = minibatch(train_data, size = compounding(4., 32., 1.001) )  #batch_size\n",
    "            print(\"batches created\")\n",
    "            # Iterate through minibatches\n",
    "            pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=737).start()  #8387 is the total number of iterations needed for an unbalanced 0.8 trainset. 0.04 of unbalanced would be approx 1540 . 4595 for lemmatized balanced. 8420 for lemmatized unbalanced\n",
    "            i = 0                                                                    # 737 for lPunctAgg and lPunctNumAgg unbalanced, 466 for balanced\n",
    "            for batch in batches:\n",
    "                texts1, labels = zip(*batch)\n",
    "                nlp.update(texts1, labels, sgd=optimizer, losses=losses, drop = next(dec))\n",
    "                i += 1\n",
    "                pbar.update(i)\n",
    "            pbar.finish()\n",
    "            with textcat.model.use_params(optimizer.averages):\n",
    "                rocs.append(evaluate_roc1(nlp, textcat))\n",
    "                output += f\"    epoch = {epoch}, losses = {losses}, roc = {rocs[-1]} \\n \"\n",
    "                print( \"epoch = \",epoch,\" losses = \", losses, \"roc = \" , rocs[-1] , \"i = \", i)\n",
    "    \n",
    "    \n",
    "    docs = [nlp.tokenizer(tex) for tex in test_texts]\n",
    "    #textcat = nlp.get_pipe(\"textcat\")\n",
    "    scores , a = textcat.predict(docs) \n",
    "    y_pred = [b[0] for b in scores] #[1 if b[0] > b[1] else 0 for b in scores]\n",
    "    roc = roc_auc_score(test_labels, y_pred)\n",
    "    print(roc)\n",
    "    \n",
    "    # create the 1000 predictions for teh logistic regeression\n",
    "\n",
    "    test_df = pd.concat([test_data_sample_m, test_data_sample_f], sort=False, verify_integrity=True)\n",
    "    df_res = pd.DataFrame({\"author\": test_df[\"author\"].tolist(), \"gender\" : y_pred, \"true_y\" : test_labels })\n",
    "    df_res.to_csv (r'Q:\\tooBigToDrive\\data-mining\\kaggle\\my_models\\spaCy\\results\\finals\\csv\\4000\\+' \"run title\"+'.csv', index = False, header=True)\n",
    "    #also save model (optional)\n",
    "    nlp.to_disk(r'Q:\\tooBigToDrive\\data-mining\\kaggle\\my_models\\spaCy\\results\\finals\\models\\bodies4000')            \n",
    "                \n",
    "    # save performance log       \n",
    "    with open(\"outputs\\\\\" + run_title + \".txt\", \"a\") as file: #name\n",
    "        file.write(run_title +\"\\n\" + output)\n",
    "        file.close()\n",
    "\n",
    "    #load the model (optional)\n",
    "    # print(\"Loading from\", output_dir)\n",
    "    #nlp2 = spacy.load(output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myEnv]",
   "language": "python",
   "name": "conda-env-myEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

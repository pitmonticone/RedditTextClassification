{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subreddits\n",
    "\n",
    "In this notebook we extract information from subreddits only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "import math   \n",
    "from spacy.util import minibatch , compounding , decaying\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from progressbar import ProgressBar, Bar, Percentage\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "# enable it to define your own optimizer (optional)\n",
    "# from spacy_transformers.util import cyclic_triangular_rate\n",
    "# from thinc.neural.optimizers import Adam\n",
    "# from thinc.neural import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_data_full = pd.read_csv(\"Q:\\\\tooBigToDrive\\data-mining\\kaggle\\data\\csv\\\\train_data.csv\")\n",
    "train_target_full = pd.read_csv(\"Q:\\\\tooBigToDrive\\data-mining\\kaggle\\data\\csv\\\\train_target.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = {}\n",
    "for author, group in train_data_full.groupby(\"author\"):\n",
    "    features_dict[author] = \" \".join(group[\"subreddit\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_dfed = {\"author\": list(features_dict.keys()) , \"subreddit\" : list(features_dict.values()) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           author                                          subreddit\n",
      "0          -Jared                       AskReddit tall pics StarWars\n",
      "1         -Peeter                                             gainit\n",
      "2        -evasian  MouseReview MechanicalKeyboards jailbreak AskR...\n",
      "3         -rubiks                                    AskWomen AskMen\n",
      "4  -true_neutral-                    mildlyinteresting todayilearned\n"
     ]
    }
   ],
   "source": [
    "author_subrdts = pd.DataFrame.from_dict(to_be_dfed)\n",
    "print(author_subrdts.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "gender = [0 for i in range(len(author_subrdts))]\n",
    "\n",
    "for idx, row in train_target_full.iterrows():\n",
    "    if row.gender == 1:\n",
    "        indexes = author_subrdts.index[author_subrdts[\"author\"] == row.author].tolist()\n",
    "        for i in indexes:\n",
    "                gender[i] += 1\n",
    "\n",
    "if(len(np.unique(gender) == 2)):\n",
    "    print(\"ok\")\n",
    "else:\n",
    "    print(\"there has been an error with gender recognition, please halt the program now\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_subrdts[\"gender\"] = gender\n",
    "print(author_subrdts.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2698"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# female fraction\n",
    "sum(gender)/len(gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = math.floor(len(author_subrdts)*0.8)\n",
    "\n",
    "seed = 100\n",
    "    \n",
    "#unbalanced\n",
    "\n",
    "train_df = author_subrdts.sample(split ,random_state=seed )\n",
    "test_df = author_subrdts.drop(train_df.index)\n",
    "\n",
    "train_texts  = train_df[\"subreddit\"].tolist()\n",
    "test_texts =  test_df[\"subreddit\"].tolist()\n",
    "\n",
    "train_labels = [{'cats': {'1': label == 1,'0': label == 0}} for label in train_df[\"gender\"].tolist()]\n",
    "\n",
    "\n",
    "\n",
    "test_labels = [i for i in test_df[\"gender\"].tolist()]  #[0 for i in range(len(train_data_full_m)-split)] + [1 for i in range(len(train_data_full_f)-split)]\n",
    "train_data  = list(zip(train_texts, train_labels))\n",
    "print(\"len(train_texts) = \",len(train_texts),\"len(test_texts) = \" , len(test_texts))\n",
    "print(\"len(test_texts) == len(test_labels) : \", len(test_texts) == len(test_labels) )\n",
    "print(\"len(train_data) = \",len(train_data), \"len(test_data) = \", len(test_texts))\n",
    "\n",
    "# end of unbalanced\n",
    "\n",
    "\n",
    "# balanced\n",
    "\n",
    "# # balanced\n",
    "# author_f = author_subrdts[author_subrdts[\"gender\"] == 1]\n",
    "# author_m = author_subrdts[author_subrdts[\"gender\"] == 0]\n",
    "# print(\"len(author_f) = \", len(author_f))\n",
    "# seed = 101\n",
    "# split = math.floor(len(author_f)*0.8)\n",
    "\n",
    "# train_f = author_f.sample(n = split, random_state = seed)\n",
    "# test_f = author_f.drop(train_f.index)\n",
    "\n",
    "# train_m = author_m.sample(n  = split, random_state = seed)\n",
    "# test_m = author_m.drop(train_m.index)\n",
    "\n",
    "# train_subs_f = train_f[\"subreddit\"].tolist()\n",
    "# train_subs_m = train_m[\"subreddit\"].tolist()\n",
    "# test_subs_f = test_f[\"subreddit\"].tolist()\n",
    "# test_subs_m = test_m[\"subreddit\"].tolist()\n",
    "\n",
    "# train_texts = train_subs_f + train_subs_m\n",
    "# train_labels = [{'cats': {'1': True ,'0': False}} for label in range(split)] + [{'cats': {'1': False ,'0': True }} for label in range(split)]\n",
    "# test_texts = test_subs_m + test_subs_f\n",
    "# test_labels = [0 for i in range(len(test_subs_m))] +  [1 for i in range(len(test_subs_f))]\n",
    "# print(\"len(train_texts) = \",len(train_texts),\"len(test_texts) = \" , len(test_texts))\n",
    "# print(\"len(test_texts) == len(test_labels) : \", len(test_texts) == len(test_labels) )\n",
    "# #test_texts, test_labels = shuffle(test_texts , test_labels , random_state = 0)\n",
    "\n",
    "# train_data = list(zip(train_texts, train_labels))\n",
    "# print(\"len(train_data) = \", len(train_data))\n",
    "# print(\"len(train_data) = \",len(train_data), \"len(test_data) = \", len(test_texts))\n",
    "\n",
    "# end of balanced\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also tried to doa bow such that it keeps informations on the number of times a puser posted under a certain subreddit. The validation performance (roc) dropped, so we exclued this possibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unbalanced\n",
    "\n",
    "# not unique\n",
    "\n",
    "# train_df_nu = author_subrdts_nu.sample(split)\n",
    "# test_df_nu = author_subrdts_nu.drop(train_df_nu.index)\n",
    "\n",
    "# train_texts_nu  = train_df_nu[\"subreddits\"].tolist()\n",
    "# test_texts_nu =  test_df_nu[\"subreddits\"].tolist()\n",
    "\n",
    "# train_labels_nu = [{'cats': {'1': label == 1,'0': label == 0}} for label in train_df_nu[\"gender\"].tolist()]\n",
    "# test_labels = [i for i in test_df_nu[\"gender\"].tolist()]  #[0 for i in range(len(train_data_full_m)-split)] + [1 for i in range(len(train_data_full_f)-split)]\n",
    "# train_data_nu  = list(zip(train_texts_nu, train_labels_nu))\n",
    "# print(\"len(train_texts_nu) = \",len(train_texts_nu),\"len(test_texts_nu) = \" , len(test_texts_nu))\n",
    "# print(\"len(test_texts_nu) == len(test_labels_nu) : \", len(test_texts_nu) == len(test_labels_nu) )\n",
    "# print(\"len(train_data_nu) = \",len(train_data_nu), \"len(test_data_nu) = \", len(test_texts_nu))\n",
    "\n",
    "# end not unique part\n",
    "\n",
    "# unique and not unique\n",
    "# train_df_nu = author_subrdts_nu.sample(split)\n",
    "# test_df = author_subrdts.drop(train_df_nu.index)\n",
    "\n",
    "# train_texts_nu  = train_df_nu[\"subreddits\"].tolist()\n",
    "# test_texts =  test_df[\"subreddits\"].tolist()\n",
    "\n",
    "# train_labels = [{'cats': {'1': label == 1,'0': label == 0}} for label in train_df_nu[\"gender\"].tolist()]\n",
    "# test_labels = [i for i in test_df_nu[\"gender\"].tolist()]\n",
    "# train_data_nuau  = list(zip(train_texts, train_labels_nu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# carica un learner che parla inglese\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "if 'textcat' not in nlp.pipe_names:\n",
    "    textcat = nlp.create_pipe(\n",
    "                  \"textcat\",\n",
    "                  config={\"exclusive_classes\": True, \"architecture\": \"ensemble\"  }) \n",
    "    nlp.add_pipe(textcat)\n",
    "else:\n",
    "    textcat = nlp.get_pipe('textcat')\n",
    "\n",
    "# create labels\n",
    "textcat.add_label(\"1\")\n",
    "textcat.add_label(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_roc(nlp,textcat):\n",
    "\n",
    "    print(\"evaluating roc \\n\")\n",
    "    docs = [nlp.tokenizer(tex) for tex in test_texts]\n",
    "\n",
    "    scores , a = textcat.predict(docs) \n",
    "    y_pred = [b[0] for b in scores]\n",
    "    roc = roc_auc_score(test_labels, y_pred)\n",
    "    return roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seeds set\n",
      "strings and lists initialized\n",
      "pipe_exceptions defined\n",
      "other_pipes defined\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  736\n",
      "evaluating roc \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0  losses =  {'textcat': 11.786419858108275} roc =  0.8218200487742267 i =  736\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  736\n",
      "evaluating roc \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  1  losses =  {'textcat': 21.595143575279508} roc =  0.8806469002695418 i =  736\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  736\n",
      "evaluating roc \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  2  losses =  {'textcat': 29.50558242625266} roc =  0.9122628674111154 i =  736\n",
      "shuffling data...\n",
      "creating batches...\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  736\n",
      "evaluating roc \n",
      "\n",
      "epoch =  3  losses =  {'textcat': 35.72911886218753} roc =  0.9205955589783084 i =  736\n",
      "Wall time: 7min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "random.seed(1)\n",
    "spacy.util.fix_random_seed(1)\n",
    "\n",
    "# train_data for unique suvredidts, trian_data_nu for not unique, train_data_nuau for not unique in train and unique in test\n",
    "train_data = train_data\n",
    "\n",
    "\n",
    "nlp.vocab.vectors.name = 'spacy_pretrained_vectors'\n",
    "\n",
    "# speaks for itself\n",
    "print(\"random seeds set\")\n",
    "\n",
    "losses = {}\n",
    "rocs = []\n",
    "run_title = \"RUN : spacy Ensemble Lemmatized Averaged drop = 0.1 , compound(500,1000,1.001), ngram_size = 2  \\n\"\n",
    "output = \"\"\n",
    "\n",
    "print(\"strings and lists initialized\")\n",
    "\n",
    "\n",
    "#learning process\n",
    "#for batch_size in [10, 50, 100, 200, 300] : \n",
    "pipe_exceptions = ['textcat'] \n",
    "print(\"pipe_exceptions defined\")\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "print(\"other_pipes defined\")\n",
    "dec = decaying(0.6 , 0.2, 1e-4)\n",
    "with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "    optimizer = nlp.begin_training()\n",
    "#     optimizer.L2 = 0.0\n",
    "#     optimizer.alpha = 0.001\n",
    "#   learn_rate=2e-5\n",
    "#     batch_size  = 4\n",
    "#     learn_rate = 0.0005\n",
    "#     learn_rates = cyclic_triangular_rate(learn_rate / 3, learn_rate * 3, 2 * len(train_data) // batch_size)\n",
    "#     nlp.begin_training()\n",
    "#     ops = Model.ops\n",
    "#     beta1 = 0.9 #0.9\n",
    "#     beta2 = 0.999 #0.999\n",
    "#     eps = 1e-10 #1e-8\n",
    "#     L2 = 0.0 #1e-6\n",
    "#     max_grad_norm = 1 #1.0\n",
    "#     optimizer = Adam(ops, learn_rate, L2=L2, beta1=beta1, beta2=beta2, eps=eps)\n",
    "#     optimizer.max_grad_norm = max_grad_norm\n",
    "#     optimizer.device = ops.device\n",
    "    for epoch in range(4):\n",
    "        print(\"shuffling data...\")\n",
    "        random.shuffle(train_data)\n",
    "        #optimizer.learn_rate = float(next(learn_rates))\n",
    "        # Create the batch generator with batch size = 8\n",
    "        print(\"creating batches...\")\n",
    "        batches = minibatch(train_data, size=compounding(4., 32., 1.001)) \n",
    "        pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=737).start()\n",
    "        i = 0\n",
    "        print(\"training...\")\n",
    "        for batch in batches:\n",
    "            texts1, labels = zip(*batch)\n",
    "            nlp.update(texts1, labels, sgd=optimizer, losses=losses, drop = next(dec)) #, drop = 0.2\n",
    "            i += 1\n",
    "            pbar.update(i)\n",
    "        pbar.finish()\n",
    "        print(\"i = \",  i)\n",
    "        with textcat.model.use_params(optimizer.averages):\n",
    "            rocs.append(evaluate_roc(nlp, textcat))\n",
    "            output += f\"    epoch = {epoch}, losses = {losses}, roc = {rocs[-1]} \\n \"\n",
    "            print( \"epoch = \",epoch,\" losses = \", losses, \"roc = \" , rocs[-1] , \"i = \", i)\n",
    "            \n",
    "        \n",
    "# with open(\"spacyEnsembleLemmatizedAveraged_output_file.txt\", \"a\") as f:\n",
    "#     f.write(run_title + output)\n",
    "#     f.close()\n",
    "\n",
    "# # save the model\n",
    "# output_dir = %pwd\n",
    "# nlp.to_disk(\"Q:\\\\tooBigToDrive\\data-mining\\kaggle\\my_models\\spacCysaved\\ensembleLA\")\n",
    "# print(\"Saved model to\", output_dir)\n",
    "\n",
    "#load it\n",
    "# print(\"Loading from\", output_dir)\n",
    "#nlp2 = spacy.load(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphs to have an idea of the predicted distribution. \"Good\"  models should return a bimodal distributioan. Also see the notebook \n",
    "docs = [nlp.tokenizer(tex) for tex in test_texts]\n",
    "scores , a = textcat.predict(docs) \n",
    "y_pred = [b[0] for b in scores]\n",
    "roc = roc_auc_score(test_labels, y_pred)\n",
    "print(roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_pred, bins=\"auto\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions for logistic\n",
    "df_res = pd.DataFrame({\"author\": test_df[\"author\"].tolist(), \"gender\" : y_pred, \"true_y\" : test_labels  })\n",
    "df_res.to_csv (r'Q:\\tooBigToDrive\\data-mining\\kaggle\\my_models\\spaCy\\results\\finals\\csv\\4000\\subsDrop_ensemble_e4.csv', index = False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myEnv]",
   "language": "python",
   "name": "conda-env-myEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

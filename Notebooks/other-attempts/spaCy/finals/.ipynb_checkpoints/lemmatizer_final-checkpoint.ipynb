{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from progressbar import ProgressBar, Bar, Percentage\n",
    "import numpy as np\n",
    "import string as stri\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q:\\anaconda\\envs\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "Q:\\anaconda\\envs\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "Q:\\anaconda\\envs\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "Q:\\anaconda\\envs\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "Q:\\anaconda\\envs\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "Q:\\anaconda\\envs\\myEnv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "nlplg  = spacy.load(r\"en_core_web_lg-2.2.5\", disable = [\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          author        subreddit   created_utc  \\\n",
      "0  ejchristian86  TwoXChromosomes  1.388534e+09   \n",
      "1      ZenDragon           gaming  1.388534e+09   \n",
      "2   savoytruffle        AskReddit  1.388534e+09   \n",
      "3   hentercenter     stlouisblues  1.388534e+09   \n",
      "4   rick-o-suave             army  1.388534e+09   \n",
      "\n",
      "                                                body  \n",
      "0  I hadn't ever heard of them before joining thi...  \n",
      "1                At 7680 by 4320 with 64x AA, right?  \n",
      "2                                            bite me  \n",
      "3                    Damn that was a good penalty :(  \n",
      "4  I swore into DEP on 6-OCT and I left 5-NOV und...  \n"
     ]
    }
   ],
   "source": [
    "final_data =  pd.read_csv(r\"test_data.csv\")\n",
    "print(final_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_subs(l):\n",
    "    s = set(l)\n",
    "    return \" \".join([st.lower() for st in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_join(a):\n",
    "    l = []\n",
    "    for b in a:\n",
    "        l.append(str(b))\n",
    "    return \" \".join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           author                                          subreddit  \\\n",
      "0    --redbeard--  insightfulquestions youshouldknow britishprobl...   \n",
      "1       -Allaina-                            fancyfollicles askwomen   \n",
      "2  -AllonsyAlonso  explainlikeimfive funny videos askmen getdisci...   \n",
      "3          -Beth-                             teenagers funny cringe   \n",
      "4        -Greeny-                guildwars2 xbox360 wtf windowsphone   \n",
      "\n",
      "                                                body  \n",
      "0  This got me through my masters degree This is ...  \n",
      "1  Around 50 pair of panties, most of them being ...  \n",
      "2  There is no escape from r/reality? I have the ...  \n",
      "3  I would have thrown up on stage.  \\nSeriously,...  \n",
      "4   http://www.windowsphone.com/s?appid=430cf007-...  \n"
     ]
    }
   ],
   "source": [
    "train_data_full_agg = final_data.groupby([\"author\"], as_index  = False).agg({'subreddit':  proc_subs, \"body\": my_join}) \n",
    "to_be_lemmed_agg = train_data_full_agg[\"body\"].tolist()\n",
    "#to_be_lemmed = final_data[\"body\"].tolist()\n",
    "print(train_data_full_agg.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21244741\n"
     ]
    }
   ],
   "source": [
    "print(max([len(t) for t in train_data_full_agg[\"body\"].tolist()]))\n",
    "#max([len(str(t)) for t in final_data[\"body\"].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessPunctNumSent(tex):\n",
    "    tex = str(tex)\n",
    "    tex = tex.replace(\"\\r\",\"\")\n",
    "    tex = tex.replace(\"\\\\n\",\".\")\n",
    "    tex = tex.replace(\"\\n\",\".\")\n",
    "    tex = tex.translate(str.maketrans(\"\",\"\", '\"#$%&\\'()*+,-/:<=>@[\\\\]^_`{|}~')).translate(str.maketrans('','','1234567890')).lower().strip()\n",
    "    tex = re.sub(r\"\\!+\", \".\", tex)\n",
    "    tex = re.sub(r\";+\", \".\", tex)\n",
    "    tex = re.sub(r\"\\?+\", \".\", tex)\n",
    "    tex = re.sub(r\"(\\.+\\s*)+\", \". \", tex)\n",
    "    tex = tex.strip()\n",
    "    tex = \" \".join(tex.split())\n",
    "    return tex\n",
    "\n",
    "\n",
    "def preprocessPunctNum(tex):\n",
    "    tex = tex.replace(\"\\r\",\"\")\n",
    "    tex = tex.replace(\"\\\\n\",\"\")   #\n",
    "    tex = tex.replace(\"\\n\",\"\")\n",
    "    tex = tex.translate(str.maketrans(\"\",\"\", stri.punctuation)).translate(str.maketrans('','','1234567890')).lower().strip()\n",
    "    tex = \" \".join(tex.split())\n",
    "    return tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|#########################################################################|\n"
     ]
    }
   ],
   "source": [
    "nlplg.max_length = 21245741\n",
    "docs = []\n",
    "j =0\n",
    "texts = to_be_lemmed_agg\n",
    "pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=len(texts)).start()\n",
    "for tex in texts:\n",
    "    docs.append(nlplg(preprocessPunctNum(tex)))\n",
    "    j += 1\n",
    "    pbar.update(j)\n",
    "pbar.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(docs))\n",
    "# print(docs[-1])\n",
    "# print(\"\\n -------------------------- \\n \",to_be_lemmed[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lik lemmetize2 but alse remove oov\n",
    "def lemmatize22(texts):\n",
    "    final_l = []\n",
    "    print(\"lemmatize21\")\n",
    "    pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=len(texts)).start()\n",
    "    i = 0\n",
    "    for doc in docs:\n",
    "        final = \"\"\n",
    "        for token in doc:\n",
    "            #if token.pos_ != \"PUNCT\" and not token.is_stop and token.tag_ != \"NNP\":\n",
    "            if not token.is_oov and token.pos_ not in [\"SPACE\"] and not token.is_stop and token.is_alpha: # and or token.text == \".\"\n",
    "                if token.text != \".\":\n",
    "                    final += token.lemma_ + \" \"\n",
    "                else:\n",
    "                    final = final.rstrip()\n",
    "                    final += \". \"\n",
    "        final_l.append(final.strip())  # re.sub(r\"(\\.+\\s*)+\", \". \", final).strip()\n",
    "        i += 1\n",
    "        pbar.update(i)\n",
    "    pbar.finish()\n",
    "    return final_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_data_full_agg1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           author                                          subreddit  \\\n",
      "0    --redbeard--  insightfulquestions youshouldknow britishprobl...   \n",
      "1       -Allaina-                            fancyfollicles askwomen   \n",
      "2  -AllonsyAlonso  explainlikeimfive funny videos askmen getdisci...   \n",
      "3          -Beth-                             teenagers funny cringe   \n",
      "4        -Greeny-                guildwars2 xbox360 wtf windowsphone   \n",
      "\n",
      "                                                body  \n",
      "0  This got me through my masters degree This is ...  \n",
      "1  Around 50 pair of panties, most of them being ...  \n",
      "2  There is no escape from r/reality? I have the ...  \n",
      "3  I would have thrown up on stage.  \\nSeriously,...  \n",
      "4   http://www.windowsphone.com/s?appid=430cf007-...  \n"
     ]
    }
   ],
   "source": [
    "train_data_full_agg1 = final_data.groupby([\"author\"], as_index  = False).agg({'subreddit':  proc_subs, \"body\": my_join}) \n",
    "print(train_data_full_agg1.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_agg = train_data_full_agg1[\"author\"].tolist()\n",
    "subreddits_agg = train_data_full_agg1[\"subreddit\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authors = final_data[\"author\"].tolist()\n",
    "# subreddits = final_data[\"subreddit\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"hello i am claudio\".count(\" \") +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmed_agg = lemmatize22(to_be_lemmed_agg)\n",
    "print(\"writing to file\")\n",
    "df = pd.DataFrame({\"author\": authors_agg , \"subreddit\" : subreddits_agg, \"body\" : lemmed_agg })\n",
    "df.to_csv(r\"lPunctNumStopLemOovAlphaAggTest.csv\", index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmed = lemmatize21(to_be_lemmed)\n",
    "# n_words = [s.count(\" \") + 1 for s in lemmed] \n",
    "# print(\"writing to file\")\n",
    "# df = pd.DataFrame({\"author\": authors , \"subreddit\" : subreddits, \"body\" : lemmed , \"n_words\": n_words})\n",
    "# df.to_csv(r\"lPunctNumStopLemOovTest.csv\", index=False)\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp  = spacy.load(r\"en_core_web_lg-2.2.5\", disable = [\"parser\",\"ner\"])\n",
    "nlp.max_length = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def_str = r\"test\"\n",
    "\n",
    "names = [\"lPunctNumStopLemOovAlphaAggTest\"]\n",
    "for s in names:\n",
    "    csvPath = def_str +\"\\\\\"+ s + \".csv\"\n",
    "    npyPath = r\"\\test\" +\"\\\\\"+ s+ \".npy\"\n",
    "    train = pd.read_csv(csvPath)\n",
    "    train.replace(to_replace = \"empty\", value = \"\", inplace = True)\n",
    "    train[\"body\"].fillna(\"\",inplace = True)\n",
    "    to_be_vectorized = train[\"body\"].tolist()\n",
    "    vectorsl = []\n",
    "    print(\"doing\"+\" \"+s+\".csv ...\", \"len(to_be_vectorized) = \",len(to_be_vectorized) )\n",
    "    pbar = ProgressBar(widgets=[Percentage(), Bar()], maxval=len(to_be_vectorized)).start()\n",
    "    i = 0\n",
    "    with nlp.disable_pipes():\n",
    "        for tex in to_be_vectorized:\n",
    "            vectorsl.append(nlp(tex).vector)\n",
    "            i += 1\n",
    "            pbar.update(i)\n",
    "    pbar.finish()\n",
    "    vectors = np.array(vectorsl)\n",
    "    np.save(npyPath,vectors)\n",
    "    print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myEnv]",
   "language": "python",
   "name": "conda-env-myEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
